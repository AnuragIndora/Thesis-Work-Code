{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e6e617-b77e-4a15-b2ee-306bdcc64531",
   "metadata": {},
   "source": [
    "# Facial Expression Detection using Graph Neural Networks\n",
    "\n",
    "## Steps Involved\n",
    "\n",
    "1. **Data Acquisition and Load the Data**  \n",
    "   Collect facial expression datasets containing landmark points and load them into your working environment.\n",
    "\n",
    "2. **Data Preprocessing**  \n",
    "   Clean the data by handling missing values, removing outliers, and ensuring the data is in the correct format for analysis.\n",
    "\n",
    "3. **Normalization (Min-Max Normalization)**  \n",
    "   Normalize the landmark points to a specific range (e.g., [0, 1]) to ensure that all features contribute equally to the model training.\n",
    "\n",
    "4. **Graph Connection (Adjacency Matrix)**  \n",
    "   Create an adjacency matrix to represent the connections between landmark points, defining the graph structure for the GNN.\n",
    "\n",
    "5. **Graph Neural Network**  \n",
    "   Design and implement a Graph Neural Network architecture suitable for processing the graph representation of facial landmarks.\n",
    "\n",
    "6. **Training and Validation**  \n",
    "   Split the dataset into training and validation sets. Train the GNN on the training set and validate its performance on the validation set.\n",
    "\n",
    "7. **Confusion Matrix and Accuracy Metrics**  \n",
    "   Evaluate the model's performance using a confusion matrix and other accuracy metrics (e.g., precision, recall, F1-score).\n",
    "\n",
    "8. **Testing**  \n",
    "   Test the trained model on a separate test dataset to assess its generalization capability.\n",
    "\n",
    "9. **Performance Evaluation**  \n",
    "   Analyze the performance of the model based on the test results, discussing strengths and weaknesses, and potential areas for improvement.\n",
    "\n",
    "## Additional Considerations\n",
    "\n",
    "- Consider data augmentation techniques to increase the diversity of the training dataset.\n",
    "- Experiment with different GNN architectures and hyperparameters to optimize performance.\n",
    "- Implement cross-validation to ensure the robustness of the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c860e3-a8a5-4fbd-bee3-cb8a6704139f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:30.692315Z",
     "start_time": "2025-04-09T08:48:19.511285Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
    "from torch.utils.data import Dataset\n",
    "# from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import Data\n",
    "from scipy.spatial import Delaunay\n",
    "import ast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f07190-f151-438b-83b5-0dd4125f9a72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:30.743623Z",
     "start_time": "2025-04-09T08:48:30.705931Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tif_df_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665277e-3388-4bb7-974e-ebcfc7f6cf7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:31.568948Z",
     "start_time": "2025-04-09T08:48:31.561131Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "emotion_list = [\"Angry\", \"Disgust\", \"Fear\", \"Happiness\", \"Neutral\", \"Sad\", \"Surprised\"]\n",
    "\n",
    "# Initialize the label encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the emotion list\n",
    "df['encoded_label'] = encoder.fit_transform(df['Labels'])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b2e81-bead-43f0-be38-6f447523a6a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:31.686624Z",
     "start_time": "2025-04-09T08:48:31.666068Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78604513-06a5-4b00-9016-af2ea2d97569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:31.755935Z",
     "start_time": "2025-04-09T08:48:31.751888Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['Landmarks'] = df['Landmarks'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa81afd-77f1-46c5-b608-92209884701f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:31.826905Z",
     "start_time": "2025-04-09T08:48:31.821505Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Initialize MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# df['Norm_Landmarks'] = None \n",
    "\n",
    "# # Loop through each row in the 'Landmarks' column\n",
    "# for i in range(len(df)):\n",
    "#     x_value, y_value = [], []\n",
    "#     normalized_data = []\n",
    "\n",
    "#     # Extract x, y pairs from the current row's landmarks\n",
    "#     for x, y in df['Landmarks'][i]:\n",
    "#         # print(f\"x = {x} and y = {y}\")\n",
    "#         x_value.append(x)\n",
    "#         y_value.append(y)\n",
    "\n",
    "#     # Normalize the x and y values\n",
    "#     x_value = np.array(x_value).reshape(-1, 1)\n",
    "#     y_value = np.array(y_value).reshape(-1, 1)\n",
    "\n",
    "#     # Apply MinMaxScaler on x and y values\n",
    "#     x_norm_value = scaler.fit_transform(x_value)\n",
    "#     y_norm_value = scaler.fit_transform(y_value)\n",
    "\n",
    "#     # Combine normalized x and y values\n",
    "#     for x_norm, y_norm in zip(x_norm_value, y_norm_value):\n",
    "#         normalized_data.append([x_norm[0], y_norm[0]])\n",
    "\n",
    "#     # Store the normalized data back into the 'Norm_Landmarks' column\n",
    "#     df.at[i, 'Norm_Landmarks'] = normalized_data\n",
    "\n",
    "# # Display the DataFrame with normalized landmarks\n",
    "# print(df['Norm_Landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922a001-f108-487e-86e9-3e762c8ec1a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:31.851105Z",
     "start_time": "2025-04-09T08:48:31.836682Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "def delaunay_triangulation(landmarks):\n",
    "    \"\"\"\n",
    "    Perform Delaunay triangulation and return triangles and edges.\n",
    "    \"\"\"\n",
    "    tri = Delaunay(landmarks)\n",
    "    triangles = tri.simplices\n",
    "    edges = set()\n",
    "    for triangle in triangles:\n",
    "        for i in range(3):\n",
    "            edge = tuple(sorted([triangle[i], triangle[(i + 1) % 3]]))\n",
    "            edges.add(edge)\n",
    "    return triangles, list(edges)\n",
    "\n",
    "def construct_adjacency_matrix(landmarks):\n",
    "    \"\"\"\n",
    "    Construct an adjacency matrix from the Delaunay triangulation of the landmarks.\n",
    "    \n",
    "    Args:\n",
    "        landmarks (ndarray): Array of 2D points (n, 2) representing the landmarks.\n",
    "    \n",
    "    Returns:\n",
    "        adjacency_matrix (ndarray): A binary adjacency matrix of shape (n, n).\n",
    "    \"\"\"\n",
    "    _, edges = delaunay_triangulation(landmarks)\n",
    "    num_landmarks = len(landmarks)\n",
    "    adjacency_matrix = np.zeros((num_landmarks, num_landmarks), dtype=int)\n",
    "    \n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        adjacency_matrix[i, j] = 1\n",
    "        adjacency_matrix[j, i] = 1  # Symmetric for undirected graph\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Example usage\n",
    "landmarks = np.array([[0, 0], [1, 0], [0, 1], [1, 1], [0.5, 0.5]])  # Sample landmarks\n",
    "adjacency_matrix = construct_adjacency_matrix(landmarks)\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66ae26-50f2-4e0f-a1d5-6171f67c9ed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:31.886525Z",
     "start_time": "2025-04-09T08:48:31.882867Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(adj_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a5668-a904-4f7a-aabd-53d1da6450eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:31.916013Z",
     "start_time": "2025-04-09T08:48:31.910624Z"
    }
   },
   "outputs": [],
   "source": [
    "# landmark_1 = np.array(df['Landmarks'][1])\n",
    "# adj_mat = construct_adjacency_matrix(landmark_1)\n",
    "# print(\"Adjacency Matrix:\")\n",
    "# print(adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54d306-aeb8-4186-9f15-db76fc5a7706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:31.937499Z",
     "start_time": "2025-04-09T08:48:31.931928Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_adjacency_matrix(A):\n",
    "    A = A + np.eye(A.shape[0])  # Add self-connections\n",
    "    D = np.diag(np.sum(A, axis=1))\n",
    "    D_inv_sqrt = np.linalg.inv(np.sqrt(D))\n",
    "    A_normalized = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return A_normalized\n",
    "\n",
    "\n",
    "# adj_mat_norm = normalize_adjacency_matrix(adj_mat)\n",
    "# print(adj_mat_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bc967-59ba-471f-ade1-5c18710b4600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:31.970830Z",
     "start_time": "2025-04-09T08:48:31.967723Z"
    }
   },
   "outputs": [],
   "source": [
    "# row_sum = [sum(row) for row in adj_mat]    \n",
    "\n",
    "# for s in row_sum:\n",
    "#     print(s, end=', ')\n",
    "\n",
    "# print(\"\\nNumber of Rows\")\n",
    "# print(len(row_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb93a72-1405-49a6-aa9a-06155efcb707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.004752Z",
     "start_time": "2025-04-09T08:48:31.995779Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['Landmarks']\n",
    "y = df['encoded_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\" training shape = > {X_train.shape}, {y_train.shape}\")\n",
    "print(f\" testing shape = > {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a8324a-650f-4e2e-8f9d-54e022df6e1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.060784Z",
     "start_time": "2025-04-09T08:48:32.049048Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
    "\n",
    "# Define the GCN-based model\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_nodes=51, num_features=2, num_classes=7):\n",
    "        super(GCNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = GCNConv(num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 256)\n",
    "        self.conv4 = GCNConv(256, 512)\n",
    "        self.conv5 = GCNConv(512, 1024)\n",
    "        self.conv6 = GCNConv(1024, 2048)\n",
    "        \n",
    "        # Pooling layers\n",
    "        self.pool1 = global_mean_pool\n",
    "        self.pool2 = global_max_pool\n",
    "        \n",
    "        # Fully connected layers after flattening\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply GCN layers with ReLU activations\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        # x = F.relu(self.conv6(x, edge_index))\n",
    "        \n",
    "        # Pooling layers\n",
    "        x = self.pool1(x, batch)  # Global mean pooling\n",
    "        # x = self.pool2(x, batch)  # Global max pooling\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example of input:\n",
    "# x (node features) shape = [num_nodes, num_features] = [68, 2]\n",
    "# edge_index (adjacency matrix) shape = [2, num_edges] - the index of connected nodes\n",
    "# batch: indicates which nodes belong to which graph (for multi-graph input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e34b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.125508Z",
     "start_time": "2025-04-09T08:48:32.110273Z"
    }
   },
   "outputs": [],
   "source": [
    "class LandmarkModel(nn.Module):\n",
    "    def __init__(self, input_size=136, hidden_size=256, num_classes=5):\n",
    "        \"\"\"\n",
    "        Initializes the model. \n",
    "        - input_size: number of features (landmarks * 2, e.g., 68 landmarks with 2 coordinates per landmark)\n",
    "        - hidden_size: size of hidden layers\n",
    "        - num_classes: number of output classes (emotions)\n",
    "        \"\"\"\n",
    "        super(LandmarkModel, self).__init__()\n",
    "\n",
    "        # Input layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # input_size: 136 for 68 landmarks * 2 (x, y)\n",
    "\n",
    "        # Hidden layers\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Output layer (5 classes for emotion classification)\n",
    "        self.fc4 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.relu(self.fc1(x))  # First hidden layer\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        \n",
    "        x = self.relu(self.fc2(x))  # Second hidden layer\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "\n",
    "        x = self.relu(self.fc3(x))  # Third hidden layer\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "\n",
    "        x = self.fc4(x)  # Output layer\n",
    "        return x  # Raw scores for each class (for later softmax or cross-entropy)\n",
    "\n",
    "# Step 2: Model Initialization\n",
    "model = LandmarkModel(input_size=102, hidden_size=256, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e826a-4c13-43f5-b762-200b2d1a9978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.156352Z",
     "start_time": "2025-04-09T08:48:32.152972Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create a list to store the data\n",
    "# data_list = []\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     # Convert landmarks from string (assuming it's stored as a string of numbers)\n",
    "#     landmarks = np.fromstring(row['Landmarks'], sep=' ').reshape(68, 2)  # Shape (68, 2)\n",
    "\n",
    "#     # Convert landmarks to tensor\n",
    "#     x = torch.tensor(landmarks, dtype=torch.float32)  # Shape [68, 2]\n",
    "\n",
    "#     # Get the adjacency matrix for this sample\n",
    "#     adjacency_matrix = generate_adjacency_matrix(row['Landmarks'])\n",
    "    \n",
    "#     # Convert adjacency matrix to edge_index (format expected by PyTorch Geometric)\n",
    "#     edge_index = torch.tensor(np.array(np.nonzero(adjacency_matrix)), dtype=torch.long)  # Shape [2, num_edges]\n",
    "    \n",
    "#     # Get the label\n",
    "#     y = torch.tensor([row['Label']], dtype=torch.long)  # Shape [1]\n",
    "    \n",
    "#     # Create Data object for PyG\n",
    "#     data = Data(x=x, edge_index=edge_index, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a194017-f025-41ed-af9a-ee58178ae59a",
   "metadata": {},
   "source": [
    "## Import Landmarks and encoded labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef504d3f-6b09-4379-a3c6-8647065432f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.194711Z",
     "start_time": "2025-04-09T08:48:32.189989Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Prepare data\n",
    "# X = df['Landmarks']\n",
    "# y = df['encoded_label']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753d430-67d8-4467-9282-011f497d244d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.226368Z",
     "start_time": "2025-04-09T08:48:32.220824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to process landmarks and adjacency matrix\n",
    "def process_landmarks_and_adjacency(landmarks_str):\n",
    "    # Parse the string to get the list of landmarks\n",
    "    landmarks = np.array(ast.literal_eval(landmarks_str))  # Convert string to list using literal_eval\n",
    "    adjacency_matrix = construct_adjacency_matrix(landmarks)  # Get adjacency matrix\n",
    "    return landmarks, adjacency_matrix\n",
    "\n",
    "# Your existing Dataset class definition remains the same:\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X.iloc[idx]\n",
    "        \n",
    "        # Process the landmarks and adjacency matrix\n",
    "        landmarks, adjacency_matrix = process_landmarks_and_adjacency(row)  # Pass the string directly\n",
    "        \n",
    "        # Convert landmarks to tensor (Shape: 68, 2)\n",
    "        x = torch.tensor(landmarks, dtype=torch.float)  \n",
    "        \n",
    "        # Convert adjacency matrix to edge_index (Shape: 2, num_edges)\n",
    "        edge_index = torch.tensor(np.array(np.nonzero(adjacency_matrix)), dtype=torch.long) \n",
    "        \n",
    "        # Get the label for this sample\n",
    "        y = torch.tensor(self.y.iloc[idx], dtype=torch.long)  # Label\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7efd2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.264424Z",
     "start_time": "2025-04-09T08:48:32.259099Z"
    }
   },
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X.iloc[idx]\n",
    "        \n",
    "        # Process the landmarks and adjacency matrix\n",
    "        landmarks, adjacency_matrix = process_landmarks_and_adjacency(row)  # Pass the string directly\n",
    "        \n",
    "        # Convert landmarks to tensor (Shape: 68, 2)\n",
    "        x = torch.tensor(landmarks, dtype=torch.float)  \n",
    "\n",
    "        \n",
    "        # Get the label for this sample\n",
    "        y = torch.tensor(self.y.iloc[idx], dtype=torch.long)  # Label\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69fd27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.316063Z",
     "start_time": "2025-04-09T08:48:32.313190Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c24a1a-3def-4a60-9a57-4f8f316d6b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.446016Z",
     "start_time": "2025-04-09T08:48:32.440271Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import ast\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch_geometric.data import Data\n",
    "\n",
    "# # Function to process landmarks and adjacency matrix\n",
    "# def process_landmarks_and_adjacency(landmarks_str):\n",
    "#     # Parse the string to get the list of landmarks\n",
    "#     print(f\"landmarks_str: {landmarks_str}\")  # Print input landmarks string\n",
    "#     landmarks = np.array(ast.literal_eval(landmarks_str))  # Convert string to list using literal_eval\n",
    "#     print(f\"landmarks (after eval): {landmarks}\")  # Print landmarks after parsing\n",
    "    \n",
    "#     adjacency_matrix = construct_adjacency_matrix(landmarks)  # Get adjacency matrix\n",
    "#     print(f\"adjacency_matrix: {adjacency_matrix}\")  # Print adjacency matrix\n",
    "    \n",
    "#     return landmarks, adjacency_matrix\n",
    "\n",
    "# # Your existing Dataset class definition remains the same:\n",
    "# class LandmarkDataset(Dataset):\n",
    "#     def __init__(self, X, y):\n",
    "#         self.X = X\n",
    "#         self.y = y\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.X.iloc[idx]\n",
    "#         print(f\"Row {idx}: {row}\")  # Print the row from X for debugging\n",
    "        \n",
    "#         # Process the landmarks and adjacency matrix\n",
    "#         landmarks, adjacency_matrix = process_landmarks_and_adjacency(row)  # Pass the string directly\n",
    "        \n",
    "#         # Convert landmarks to tensor (Shape: 68, 2)\n",
    "#         x = torch.tensor(landmarks, dtype=torch.float)  \n",
    "#         print(f\"x (landmarks as tensor): {x.shape} {x}\")  # Print tensor of landmarks\n",
    "        \n",
    "#         # Convert adjacency matrix to edge_index (Shape: 2, num_edges)\n",
    "#         edge_index = torch.tensor(np.array(np.nonzero(adjacency_matrix)), dtype=torch.long) \n",
    "#         print(f\"edge_index (nonzero positions): {edge_index.shape} {edge_index}\")  # Print edge_index\n",
    "        \n",
    "#         # Get the label for this sample\n",
    "#         y = torch.tensor(self.y.iloc[idx], dtype=torch.long)  # Label\n",
    "#         print(f\"y (label): {y}\")  # Print label\n",
    "        \n",
    "#         return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# # Create DataLoader from PyTorch Geometric\n",
    "# train_dataset = LandmarkDataset(X_train, y_train)\n",
    "# test_dataset = LandmarkDataset(X_test, y_test)\n",
    "\n",
    "# # Use the PyTorch Geometric DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # Debugging the DataLoader and the first batch\n",
    "# for batch in train_loader:\n",
    "#     print(f\"Batch x: {batch.x.shape}, Batch edge_index: {batch.edge_index.shape}, Batch y: {batch.y.shape}\")\n",
    "#     break  # Print only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323fe3ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.524609Z",
     "start_time": "2025-04-09T08:48:32.521186Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26500d4-2e43-4047-a145-6b160f8b4b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.577640Z",
     "start_time": "2025-04-09T08:48:32.567803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataLoader from PyTorch Geometric\n",
    "train_dataset = LandmarkDataset(X_train, y_train)\n",
    "test_dataset = LandmarkDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "# Use the PyTorch Geometric DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Debugging the DataLoader and the first batch\n",
    "# for batch in train_loader:\n",
    "#     print(f\"Batch x: {batch.x.shape}, Batch edge_index: {batch.edge_index.shape}, Batch y: {batch.y.shape}\")\n",
    "#     break  # Print only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441b35c-7244-4fee-a206-84eac5ae3f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.638960Z",
     "start_time": "2025-04-09T08:48:32.634753Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Debugging the full train_loader\n",
    "# for batch_idx, batch in enumerate(train_loader):\n",
    "#     print(f\"\\nBatch {batch_idx + 1}:\")\n",
    "#     print(f\"x shape: {batch.x.shape}, x: {batch.x}\")\n",
    "#     # print(f\"edge_index shape: {batch.edge_index.shape}, edge_index: {batch.edge_index}\")\n",
    "#     # print(f\"y shape: {batch.y.shape}, y: {batch.y}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06802b1-2653-45b5-8437-42f6c744224f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.687125Z",
     "start_time": "2025-04-09T08:48:32.673612Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5352efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.807114Z",
     "start_time": "2025-04-09T08:48:32.798963Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)  # Probabilities of the correct class\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Use Focal Loss instead of CrossEntropyLoss\n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e457f4-5375-489b-8d00-de29c3893019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.885112Z",
     "start_time": "2025-04-09T08:48:32.879841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "# model = GCNModel(num_nodes=51, num_features=2, num_classes=7).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155f84b-4f32-4e46-af3c-63452233f4fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:32.933185Z",
     "start_time": "2025-04-09T08:48:32.924303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(out, batch.y)\n",
    "                \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # total_loss += loss.item()\n",
    "        train_loss += loss.item() * batch.num_graphs  # Loss for the batch\n",
    "\n",
    "        # Collect predictions and labels for metrics\n",
    "        _, preds = torch.max(out, dim=1)\n",
    "        all_labels.append(batch.y.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)  # Average loss over the dataset\n",
    "\n",
    "    # Flatten lists\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\") #, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return train_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750fcff-5d01-4916-a342-a1f49e0a509e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:48:34.800546Z",
     "start_time": "2025-04-09T08:48:32.969484Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Initialize model, optimizer, loss function, etc.\n",
    "    # model = GCNModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Put the model on device (GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize the DataLoader and training loop\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for epoch in range(1, 101):  # 100 epochs\n",
    "            # print(f\"Epoch {epoch}\")\n",
    "    \n",
    "            # Train\n",
    "            train_loss, accuracy, precision, recall, f1 = train(model, train_loader, optimizer, criterion, device)\n",
    "            train_losses.append(train_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "    print(\"train_loss\")\n",
    "    print (train_loss)\n",
    "    print(\"accuracies\")\n",
    "    print(accuracies)\n",
    "    print(\"precison\")\n",
    "    print(precision)\n",
    "    print(\"recalls\")\n",
    "    print(recalls)\n",
    "    print(\"f1_Score\")\n",
    "    print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa60f26-828b-460f-80e3-5fbf7c95f23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6655ab-c03c-4b69-8f3d-cf54b4199fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406bfe0-28f5-4843-9d5f-898344e2fd32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471c657-4601-4329-831e-33754436bf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98df48-d63f-49cb-8f48-f82c089b9715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127d9c8-332d-43e7-9eaa-c07512b2f891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5935613-9408-4d13-b503-e99be61259c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import Data\n",
    "from scipy.spatial import Delaunay\n",
    "import ast \n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warning by category (e.g., deprecation warnings)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*deprecated.*\")\n",
    "\n",
    "# Load the dataframe\n",
    "df = pd.read_csv('tif_df_train.csv')\n",
    "\n",
    "# Initialize the label encoder\n",
    "emotion_list = [\"Angry\", \"Disgust\", \"Fear\", \"Happiness\", \"Neutral\", \"Sad\", \"Surprised\"]\n",
    "encoder = LabelEncoder()\n",
    "df['encoded_label'] = encoder.fit_transform(df['Labels'])\n",
    "\n",
    "# Function to perform Delaunay triangulation\n",
    "def delaunay_triangulation(landmarks):\n",
    "    tri = Delaunay(landmarks)\n",
    "    triangles = tri.simplices\n",
    "    edges = set()\n",
    "    for triangle in triangles:\n",
    "        for i in range(3):\n",
    "            edge = tuple(sorted([triangle[i], triangle[(i + 1) % 3]]))\n",
    "            edges.add(edge)\n",
    "    return triangles, list(edges)\n",
    "\n",
    "# Function to construct adjacency matrix from Delaunay triangulation\n",
    "def construct_adjacency_matrix(landmarks):\n",
    "    _, edges = delaunay_triangulation(landmarks)\n",
    "    num_landmarks = len(landmarks)\n",
    "    adjacency_matrix = np.zeros((num_landmarks, num_landmarks), dtype=int)\n",
    "    \n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        adjacency_matrix[i, j] = 1\n",
    "        adjacency_matrix[j, i] = 1  # Symmetric for undirected graph\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "# # Convert the landmarks into numpy arrays and adjacency matrices\n",
    "# def process_landmarks_and_adjacency(row):\n",
    "#     landmarks = np.fromstring(row['Landmarks'], sep=' ').reshape(68, 2)  # Shape (68, 2)\n",
    "#     adjacency_matrix = construct_adjacency_matrix(landmarks)  # Get adjacency matrix\n",
    "#     return landmarks, adjacency_matrix\n",
    "\n",
    "# Prepare data\n",
    "X = df['Landmarks']\n",
    "y = df['encoded_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to process landmarks and adjacency matrix\n",
    "def process_landmarks_and_adjacency(landmarks_str):\n",
    "    # Parse the string to get the list of landmarks\n",
    "    landmarks = np.array(ast.literal_eval(landmarks_str))  # Convert string to list using literal_eval\n",
    "    adjacency_matrix = construct_adjacency_matrix(landmarks)  # Get adjacency matrix\n",
    "    return landmarks, adjacency_matrix\n",
    "\n",
    "# Your existing Dataset class definition remains the same:\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X.iloc[idx]\n",
    "        \n",
    "        # Process the landmarks and adjacency matrix\n",
    "        landmarks, adjacency_matrix = process_landmarks_and_adjacency(row)  # Pass the string directly\n",
    "        \n",
    "        # Convert landmarks to tensor (Shape: 68, 2)\n",
    "        x = torch.tensor(landmarks, dtype=torch.float)  \n",
    "        \n",
    "        # Convert adjacency matrix to edge_index (Shape: 2, num_edges)\n",
    "        edge_index = torch.tensor(np.array(np.nonzero(adjacency_matrix)), dtype=torch.long) \n",
    "        \n",
    "        # Get the label for this sample\n",
    "        y = torch.tensor(self.y.iloc[idx], dtype=torch.long)  # Label\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "        \n",
    "# Create DataLoader from PyTorch Geometric\n",
    "train_dataset = LandmarkDataset(X_train, y_train)\n",
    "test_dataset = LandmarkDataset(X_test, y_test)\n",
    "\n",
    "# Use the PyTorch Geometric DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the GCN-based model\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_nodes=68, num_features=2, num_classes=7):\n",
    "        super(GCNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = GCNConv(num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 256)\n",
    "        self.conv4 = GCNConv(256, 512)\n",
    "        self.conv5 = GCNConv(512, 1024)\n",
    "        self.conv6 = GCNConv(1024, 2048)\n",
    "        \n",
    "        # Fully connected layers after flattening\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply GCN layers with ReLU activations\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        \n",
    "        # Pooling layers - applying both mean and max pooling\n",
    "        x_mean = global_mean_pool(x, batch)  # Global mean pooling\n",
    "        x_max = global_max_pool(x, batch)    # Global max pooling\n",
    "        \n",
    "        # Concatenate the pooled results\n",
    "        x = torch.cat([x_mean, x_max], dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Initialize model\n",
    "model = GCNModel(num_nodes=51, num_features=2, num_classes=7).to(device)\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(out, batch.y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Testing loop\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            _, predicted = torch.max(out, dim=1)\n",
    "            correct += (predicted == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Training and evaluation\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_acc = test(model, test_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c03461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Training and evaluation\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    y_true, y_pred = test(model, test_loader)\n",
    "    test_acc = (np.array(y_true) == np.array(y_pred)).mean() * 100\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.2f}%')\n",
    "\n",
    "    if epoch % 10 == 0:  # Every 10 epochs, plot confusion matrix\n",
    "        plot_confusion_matrix(y_true, y_pred, emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20926b-71ad-48af-8016-d3f7366708c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Define the GCN-based model\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_nodes=68, num_features=2, num_classes=7):\n",
    "        super(GCNModel, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = GCNConv(num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 256)\n",
    "        self.conv4 = GCNConv(256, 512)\n",
    "        self.conv5 = GCNConv(512, 1024)\n",
    "\n",
    "        # Fully connected layers after flattening\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply GCN layers with ReLU activations\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "\n",
    "        # Pooling layers\n",
    "        x = global_mean_pool(x, batch)  # Global mean pooling\n",
    "        # x = global_max_pool(x, batch)  # Global max pooling\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Training function with loss curve\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move to device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * batch.num_graphs  # Loss for the batch\n",
    "\n",
    "        # Collect predictions and labels for metrics\n",
    "        _, preds = torch.max(out, dim=1)\n",
    "        all_labels.append(batch.y.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)  # Average loss over the dataset\n",
    "\n",
    "    # Flatten lists\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return train_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Evaluation function with confusion matrix\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            test_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "            # Collect predictions and labels for metrics\n",
    "            _, preds = torch.max(out, dim=1)\n",
    "            all_labels.append(batch.y.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Average loss over the dataset\n",
    "\n",
    "    # Flatten lists\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return test_loss, accuracy, precision, recall, f1, cm\n",
    "\n",
    "\n",
    "# Main code to train and evaluate the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model, optimizer, loss function, etc.\n",
    "    model = GCNModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Put the model on device (GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize the DataLoader and training loop\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Assuming you have `train_loader` and `test_loader` already defined\n",
    "    for epoch in range(1, 101):  # 100 epochs\n",
    "        print(f\"Epoch {epoch}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, accuracy, precision, recall, f1 = train(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # Evaluate\n",
    "        test_loss, accuracy, precision, recall, f1, cm = evaluate(model, test_loader, criterion, device)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        # # Plot loss curves\n",
    "        # plt.plot(train_losses, label='Train Loss')\n",
    "        # plt.plot(test_losses, label='Test Loss')\n",
    "        # plt.xlabel('Epochs')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.legend()\n",
    "        # plt.title('Loss Curve')\n",
    "        # plt.show()\n",
    "\n",
    "        # # Confusion matrix\n",
    "        # plt.figure(figsize=(10, 7))\n",
    "        # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=emotion_list, yticklabels=emotion_list)\n",
    "        # plt.xlabel('Predicted')\n",
    "        # plt.ylabel('True')\n",
    "        # plt.title('Confusion Matrix')\n",
    "        # plt.show()\n",
    "\n",
    "    # Plotting performance metrics over epochs\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(accuracies, label='Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(precisions, label='Precision')\n",
    "    plt.title('Precision Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(recalls, label='Recall')\n",
    "    plt.title('Recall Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(f1_scores, label='F1 Score')\n",
    "    plt.title('F1 Score Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568a6ed-6f51-4e3a-920e-fb401f81fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import Data\n",
    "from scipy.spatial import Delaunay\n",
    "import ast\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warning by category (e.g., deprecation warnings)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*deprecated.*\")\n",
    "\n",
    "# Load the dataframe\n",
    "df = pd.read_csv('tif_df_train.csv')\n",
    "\n",
    "# Initialize the label encoder\n",
    "emotion_list = [\"Angry\", \"Disgust\", \"Fear\", \"Happiness\", \"Neutral\", \"Sad\", \"Surprised\"]\n",
    "encoder = LabelEncoder()\n",
    "df['encoded_label'] = encoder.fit_transform(df['Labels'])\n",
    "\n",
    "# Function to perform Delaunay triangulation\n",
    "def delaunay_triangulation(landmarks):\n",
    "    tri = Delaunay(landmarks)\n",
    "    triangles = tri.simplices\n",
    "    edges = set()\n",
    "    for triangle in triangles:\n",
    "        for i in range(3):\n",
    "            edge = tuple(sorted([triangle[i], triangle[(i + 1) % 3]]))\n",
    "            edges.add(edge)\n",
    "    return triangles, list(edges)\n",
    "\n",
    "# Function to construct adjacency matrix from Delaunay triangulation\n",
    "def construct_adjacency_matrix(landmarks):\n",
    "    _, edges = delaunay_triangulation(landmarks)\n",
    "    num_landmarks = len(landmarks)\n",
    "    adjacency_matrix = np.zeros((num_landmarks, num_landmarks), dtype=int)\n",
    "    \n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        adjacency_matrix[i, j] = 1\n",
    "        adjacency_matrix[j, i] = 1  # Symmetric for undirected graph\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Function to process landmarks and adjacency matrix\n",
    "def process_landmarks_and_adjacency(landmarks_str):\n",
    "    # Parse the string to get the list of landmarks\n",
    "    landmarks = np.array(ast.literal_eval(landmarks_str))  # Convert string to list using literal_eval\n",
    "    adjacency_matrix = construct_adjacency_matrix(landmarks)  # Get adjacency matrix\n",
    "    return landmarks, adjacency_matrix\n",
    "\n",
    "# Prepare data\n",
    "X = df['Landmarks']\n",
    "y = df['encoded_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset class for PyTorch\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X.iloc[idx]\n",
    "        \n",
    "        # Process the landmarks and adjacency matrix\n",
    "        landmarks, adjacency_matrix = process_landmarks_and_adjacency(row)  # Pass the string directly\n",
    "        \n",
    "        # Convert landmarks to tensor (Shape: 68, 2)\n",
    "        x = torch.tensor(landmarks, dtype=torch.float)  \n",
    "        \n",
    "        # Convert adjacency matrix to edge_index (Shape: 2, num_edges)\n",
    "        edge_index = torch.tensor(np.array(np.nonzero(adjacency_matrix)), dtype=torch.long) \n",
    "        \n",
    "        # Get the label for this sample\n",
    "        y = torch.tensor(self.y.iloc[idx], dtype=torch.long)  # Label\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Create DataLoader from PyTorch Geometric\n",
    "train_dataset = LandmarkDataset(X_train, y_train)\n",
    "test_dataset = LandmarkDataset(X_test, y_test)\n",
    "\n",
    "# Use the PyTorch Geometric DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the GCN-based model for multi-label classification\n",
    "# Define the GCN-based model\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_nodes=68, num_features=2, num_classes=7):\n",
    "        super(GCNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = GCNConv(num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 256)\n",
    "        self.conv4 = GCNConv(256, 512)\n",
    "        self.conv5 = GCNConv(512, 1024)\n",
    "        self.conv6 = GCNConv(1024, 2048)\n",
    "        \n",
    "        # Fully connected layers after flattening\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply GCN layers with ReLU activations\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        \n",
    "        # Pooling layers - applying both mean and max pooling\n",
    "        x_mean = global_mean_pool(x, batch)  # Global mean pooling\n",
    "        x_max = global_max_pool(x, batch)    # Global max pooling\n",
    "        \n",
    "        # Concatenate the pooled results\n",
    "        x = torch.cat([x_mean, x_max], dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Training function with loss curve\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move to device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch.num_graphs  # Loss for the batch\n",
    "        \n",
    "        # Collect predictions and labels for metrics\n",
    "        preds = torch.sigmoid(out)  # Sigmoid for multi-label classification\n",
    "        all_labels.append(batch.y.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)  # Average loss over the dataset\n",
    "    \n",
    "    # Flatten lists\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    \n",
    "    # Threshold predictions at 0.5 for multi-label classification\n",
    "    all_preds = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return train_loss, accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluation function with confusion matrix\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            test_loss += loss.item() * batch.num_graphs\n",
    "            \n",
    "            # Collect predictions and labels for metrics\n",
    "            preds = torch.sigmoid(out)  # Sigmoid for multi-label classification\n",
    "            all_labels.append(batch.y.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)  # Average loss over the dataset\n",
    "    \n",
    "    # Flatten lists\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    \n",
    "    # Threshold predictions at 0.5 for multi-label classification\n",
    "    all_preds = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels.argmax(axis=1), all_preds.argmax(axis=1))\n",
    "    \n",
    "    return test_loss, accuracy, precision, recall, f1, cm\n",
    "\n",
    "# Main code to train and evaluate the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model, optimizer, loss function, etc.\n",
    "    model = GCNModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # For multi-label classification, use BCEWithLogitsLoss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Put the model on device (GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize the DataLoader and training loop\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Training and evaluation for 100 epochs\n",
    "    for epoch in range(1, 101):  # 100 epochs\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, accuracy, precision, recall, f1 = train(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, accuracy, precision, recall, f1, cm = evaluate(model, test_loader, criterion, device)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Plot loss curves\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss Curve')\n",
    "        plt.show()\n",
    "        \n",
    "        # Confusion matrix\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=emotion_list, yticklabels=emotion_list)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "    # Plotting performance metrics over epochs\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(accuracies, label='Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(precisions, label='Precision')\n",
    "    plt.title('Precision Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(recalls, label='Recall')\n",
    "    plt.title('Recall Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(f1_scores, label='F1 Score')\n",
    "    plt.title('F1 Score Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec9d79-4d5c-42f7-be60-7f8c9a651ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
