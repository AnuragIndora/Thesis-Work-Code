{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799a76cc-9bf7-4696-8d3e-6c2d255b02ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fear', 'Sad', 'Angry', 'Test', 'Neutral', 'unknow expression', 'Surprised', 'Happiness', '.ipynb_checkpoints', 'Train', 'Disgust']\n",
      "Fear = 33\n",
      "Sad = 70\n",
      "Angry = 53\n",
      "Test = 0\n",
      "Neutral = 54\n",
      "unknow expression = 8\n",
      "Surprised = 91\n",
      "Happiness = 98\n",
      ".ipynb_checkpoints = 0\n",
      "Train = 0\n",
      "Disgust = 43\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset = 'Dataset/ChildData/Croped/'\n",
    "print(os.listdir(dataset))\n",
    "\n",
    "classes = [folder for folder in os.listdir(dataset) if os.path.isdir(os.path.join(dataset, folder))]\n",
    "count = []\n",
    "\n",
    "for folder in classes:\n",
    "    folder_path = os.path.join(dataset, folder)\n",
    "    num_image = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "    count.append(num_image)\n",
    "    print(f\"{folder} = {num_image}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698dd3cc-65a7-44a9-83a3-96054b489408",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_needs = ['Fear', 'Sad', 'Angry', 'Neutral', 'Surprised', 'Disgust', 'Sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e4007c0-aa10-490c-a3a1-c82648e73403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fear', 'Sad', 'Angry', 'Neutral', 'Surprised', 'Happiness', 'Disgust']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset = 'Dataset/ChildData/Croped/Train'\n",
    "print(os.listdir(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cededd1a-b994-497a-9feb-e3a9323720e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fear = 33\n",
      "Sad = 65\n",
      "Angry = 42\n",
      "Neutral = 50\n",
      "Surprised = 72\n",
      "Happiness = 78\n",
      "Disgust = 42\n"
     ]
    }
   ],
   "source": [
    "classes = [folder for folder in os.listdir(dataset) if os.path.isdir(os.path.join(dataset, folder))]\n",
    "count = []\n",
    "\n",
    "for folder in classes:\n",
    "    folder_path = os.path.join(dataset, folder)\n",
    "    num_image = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "    count.append(num_image)\n",
    "    print(f\"{folder} = {num_image}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c196dfa9-7ea2-425b-80fa-b587863f6f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fear', 'Sad', 'Angry', 'Neutral', 'Surprised', 'Happiness', 'Disgust']\n",
      "Fear = 33\n",
      "Sad = 65\n",
      "Angry = 42\n",
      "Neutral = 50\n",
      "Surprised = 72\n",
      "Happiness = 78\n",
      "Disgust = 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset = 'Dataset/ChildData/Croped/Train/'\n",
    "print(os.listdir(dataset))\n",
    "\n",
    "classes = [folder for folder in os.listdir(dataset) if os.path.isdir(os.path.join(dataset, folder))]\n",
    "count = []\n",
    "\n",
    "for folder in classes:\n",
    "    folder_path = os.path.join(dataset, folder)\n",
    "    num_image = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "    count.append(num_image)\n",
    "    print(f\"{folder} = {num_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef742ec-578f-4cc2-b668-ab105955855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fear', 'Sad', 'Angry', 'Neutral', 'Surprised', 'Happiness', 'Disgust']\n",
      "Fear = 14\n",
      "Sad = 23\n",
      "Angry = 11\n",
      "Neutral = 18\n",
      "Surprised = 19\n",
      "Happiness = 20\n",
      "Disgust = 17\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset = 'Dataset/ChildData/Croped/Test/'\n",
    "print(os.listdir(dataset))\n",
    "\n",
    "classes = [folder for folder in os.listdir(dataset) if os.path.isdir(os.path.join(dataset, folder))]\n",
    "count = []\n",
    "\n",
    "for folder in classes:\n",
    "    folder_path = os.path.join(dataset, folder)\n",
    "    num_image = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "    count.append(num_image)\n",
    "    print(f\"{folder} = {num_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f30f39-5080-4aa9-b3c4-d3596b1e74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 14:27:49.242661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-09 14:27:49.481168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-09 14:27:49.528649: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 14:27:49.713599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 14:27:53.868807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tt\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6cb882c-ad08-4e33-99cb-300790208646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f91d5d6-dd21-4d74-a0e6-e7ff98888ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the data transformations\n",
    "# train_tt = tt.Compose([\n",
    "#     tt.RandomPerspective(distortion_scale=0.6, p=0.5),\n",
    "#     tt.RandomRotation(degrees=(0, 180)),\n",
    "#     tt.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
    "#     tt.RandomAdjustSharpness(0, p=0.25),\n",
    "#     tt.RandomAdjustSharpness(2, p=0.25),\n",
    "#     tt.RandomAutocontrast(p=0.2),\n",
    "#     tt.RandomEqualize(p=0.2),\n",
    "#     tt.Resize((48, 48)),\n",
    "#     tt.RandomHorizontalFlip(),\n",
    "#     tt.ToTensor(),\n",
    "#     tt.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd9bbd0-f005-46db-a796-c295e0a3596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'Dataset/ChildData/Croped/Train/'\n",
    "test_path = 'Dataset/ChildData/Croped/Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f95b48-bab6-4e0a-a5fc-04802f32993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (str): Path to the dataset (train or test).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all class subfolders (e.g., 'Angry', 'Disgust', etc.)\n",
    "        self.class_names = sorted(os.listdir(data_path))  # Assumes class names are folder names\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.class_names)}\n",
    "        \n",
    "        # Get image file paths along with corresponding labels\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name in self.class_names:\n",
    "            class_folder = os.path.join(data_path, class_name)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for fname in os.listdir(class_folder):\n",
    "                    if fname.endswith('.jpg'):\n",
    "                        self.image_paths.append(os.path.join(class_folder, fname))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of images in the dataset\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load an image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get the label (index of the class)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply the transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd0cf80-3f67-4c2e-ac4a-30f90b0d134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a fixed size (e.g., 224x224)\n",
    "    transforms.ToTensor(),  # Convert image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize to pre-trained model standards\n",
    "])\n",
    "\n",
    "# Initialize dataset and dataloaders for training and testing\n",
    "train_dataset = CustomDataset(data_path='Dataset/ChildData/Croped/Train/', transform=transform)\n",
    "test_dataset = CustomDataset(data_path='Dataset/ChildData/Croped/Test/', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5b9480f-e514-4c5a-93be-81087861cbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 224, 224]), torch.Size([32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20d13d51-492e-4d26-8336-3e8b34fe4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(train_path, transform=train_tt),\n",
    "#     batch_size=32,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(test_path, transform=train_tt),\n",
    "#     batch_size=32,\n",
    "#     shuffle=False  # Test data should not be shuffled\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd8579c4-1ed8-4987-ad9e-cf53fedaf2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Angry', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sad', 'Surprised']\n"
     ]
    }
   ],
   "source": [
    "root = pathlib.Path(train_path)\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157bb60-f75a-4c52-9890-1b32facad591",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70ce6df8-7434-4785-9ed3-327b9d25f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self, num_classes=7):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "\n",
    "#         self.fc = nn.Linear(in_features=32 * 24 * 24, out_features=num_classes)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         output = self.conv1(input)\n",
    "#         output = self.bn1(output)\n",
    "#         output = self.relu1(output)\n",
    "#         output = self.pool(output)\n",
    "\n",
    "#         output = self.conv2(output)\n",
    "#         output = self.relu2(output)\n",
    "\n",
    "#         output = self.conv3(output)\n",
    "#         output = self.bn3(output)\n",
    "#         output = self.relu3(output)\n",
    "\n",
    "#         output = output.view(-1, 32 * 24 * 24)\n",
    "#         output = self.fc(output)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c42de14d-5410-4196-9b46-bdb086a7783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = ConvNet(num_classes=7)\n",
    "# model = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf9e8493-c3c5-46d9-b322-c8a9e4ca29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# num_epochs = 50\n",
    "# writer = SummaryWriter('runs/childnet_exp')\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6be3d29b-d856-4d12-8e7e-7d6234255520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_count = len(glob.glob(train_path + '/**/*.jpg'))\n",
    "# test_count = len(glob.glob(test_path + '/**/*.jpg'))\n",
    "# print(train_count, test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f15fd5c0-defe-442c-9135-8968da59756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(epochs, train_loader, val_loader, criterion, optimizer, device, writer):\n",
    "#     best_accuracy = 0\n",
    "#     print(\"======================== Start Training ======================\")\n",
    "#     for e in range(epochs):\n",
    "#         model.train()  # Set model to training mode\n",
    "#         train_loss = 0\n",
    "#         validation_loss = 0\n",
    "#         train_correct = 0\n",
    "#         val_correct = 0\n",
    "\n",
    "#         # Train the model\n",
    "#         for data, labels in train_loader:\n",
    "#             data, labels = data.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             train_loss += loss.item()\n",
    "#             _, preds = torch.max(output, 1)\n",
    "#             train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "#         # Validate the model\n",
    "#         model.eval()  # Set model to evaluation mode\n",
    "#         with torch.no_grad():  # Disable gradient calculation\n",
    "#             for data, labels in val_loader:\n",
    "#                 data, labels = data.to(device), labels.to(device)\n",
    "#                 val_outputs = model(data)\n",
    "#                 val_loss = criterion(val_outputs, labels)\n",
    "#                 validation_loss += val_loss.item()\n",
    "#                 _, val_preds = torch.max(val_outputs, 1)\n",
    "#                 val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "#         train_loss = train_loss / train_count\n",
    "#         train_acc = train_correct.double() / train_count\n",
    "#         validation_loss = validation_loss / test_count\n",
    "#         val_acc = val_correct.double() / test_count\n",
    "\n",
    "#         # Log to TensorBoard\n",
    "#         writer.add_scalar('Train Loss', train_loss, e)\n",
    "#         writer.add_scalar('Train Accuracy', train_acc, e)\n",
    "#         writer.add_scalar('Validation Loss', validation_loss, e)\n",
    "#         writer.add_scalar('Validation Accuracy', val_acc, e)\n",
    "\n",
    "#         print(f\"Epoch {e+1}/{epochs}: \"\n",
    "#               f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc*100:.2f}%, \"\n",
    "#               f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "#         if val_acc > best_accuracy:\n",
    "#             torch.save(model.state_dict(), 'best_checkpoint_test.model')\n",
    "#             best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fe7afb7-aabe-41cc-bffa-351db924f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(num_epochs, train_loader, test_loader, criterion, optimizer, device, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c0886-3b56-43b3-9552-89777bcbb604",
   "metadata": {},
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "497d1b13-5a1c-419b-b09f-512f40f77a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the data transformations\n",
    "# train_tt = tt.Compose([\n",
    "#     tt.RandomPerspective(distortion_scale=0.6, p=0.5),\n",
    "#     tt.RandomRotation(degrees=(0, 180)),\n",
    "#     tt.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
    "#     tt.RandomAdjustSharpness(0, p=0.25),\n",
    "#     tt.RandomAdjustSharpness(2, p=0.25),\n",
    "#     tt.RandomAutocontrast(p=0.2),\n",
    "#     tt.RandomEqualize(p=0.2),\n",
    "#     tt.Resize((224, 224)),  # ResNet-50 typically uses 224x224 image size\n",
    "#     tt.RandomHorizontalFlip(),\n",
    "#     tt.ToTensor(),\n",
    "#     tt.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Standard ImageNet normalization\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dd8f3b9-aa31-45d5-adbd-2ae6351eead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = 'Dataset/ChildNet/Dataset/Train/'\n",
    "# test_path = 'Dataset/ChildNet/Dataset/Test/'\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(train_path, transform=train_tt),\n",
    "#     batch_size=32,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     torchvision.datasets.ImageFolder(test_path, transform=train_tt),\n",
    "#     batch_size=32,\n",
    "#     shuffle=False  # Test data should not be shuffled\n",
    "# )\n",
    "\n",
    "# root = pathlib.Path(train_path)\n",
    "# classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "# print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35537cdd-6b8a-4311-8d7e-83c6a6dfa25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/dgx1575/20je0167_anurag/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/dgx1575/20je0167_anurag/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /raid/home/dgx1575/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:01<00:00, 70.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382 122\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ResNet-50 model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze early layers (optional, for transfer learning)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer to match the number of classes\n",
    "model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "\n",
    "# Move model to the device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set optimizer for the final fully connected layer only (fine-tuning)\n",
    "optimizer = Adam(model.fc.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "num_epochs = 100\n",
    "writer = SummaryWriter('runs/childnet_resnet50')\n",
    "criterion = nn.CrossEntropyLoss()  # can change this loss\n",
    "\n",
    "train_count = len(glob.glob(train_path + '/**/*.jpg'))\n",
    "test_count = len(glob.glob(test_path + '/**/*.jpg'))\n",
    "print(train_count, test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9341cdf-8858-429d-9c1e-3e28ae8b0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(epochs, train_loader, val_loader, criterion, optimizer, device, writer):\n",
    "#     best_accuracy = 0\n",
    "#     print(\"======================== Start Training ======================\")\n",
    "#     for e in range(epochs):\n",
    "#         model.train()  # Set model to training mode\n",
    "#         train_loss = 0\n",
    "#         validation_loss = 0\n",
    "#         train_correct = 0\n",
    "#         val_correct = 0\n",
    "\n",
    "#         # Train the model\n",
    "#         for data, labels in train_loader:\n",
    "#             data, labels = data.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             train_loss += loss.item()\n",
    "#             _, preds = torch.max(output, 1)\n",
    "#             train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "#         # Validate the model\n",
    "#         model.eval()  # Set model to evaluation mode\n",
    "#         with torch.no_grad():  # Disable gradient calculation\n",
    "#             for data, labels in val_loader:\n",
    "#                 data, labels = data.to(device), labels.to(device)\n",
    "#                 val_outputs = model(data)\n",
    "#                 val_loss = criterion(val_outputs, labels)\n",
    "#                 validation_loss += val_loss.item()\n",
    "#                 _, val_preds = torch.max(val_outputs, 1)\n",
    "#                 val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "#         train_loss = train_loss / train_count\n",
    "#         train_acc = train_correct.double() / train_count\n",
    "#         validation_loss = validation_loss / test_count\n",
    "#         val_acc = val_correct.double() / test_count\n",
    "\n",
    "#         # Log to TensorBoard\n",
    "#         writer.add_scalar('Train Loss', train_loss, e)\n",
    "#         writer.add_scalar('Train Accuracy', train_acc, e)\n",
    "#         writer.add_scalar('Validation Loss', validation_loss, e)\n",
    "#         writer.add_scalar('Validation Accuracy', val_acc, e)\n",
    "\n",
    "#         print(f\"Epoch {e+1}/{epochs}: \"\n",
    "#               f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc*100:.2f}%, \"\n",
    "#               f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "#         if val_acc > best_accuracy:\n",
    "#             torch.save(model.state_dict(), 'best_checkpoint_resnet50.model')\n",
    "#             best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "232f3f26-230c-49c1-b182-9e674c721812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "def train(epochs, train_loader, val_loader, criterion, optimizer, device, writer, model):\n",
    "    best_accuracy = 0\n",
    "    train_losses, val_losses = [], []  # For plotting learning curves\n",
    "    train_accuracies, val_accuracies = [], []  # For plotting learning curves\n",
    "\n",
    "    print(\"======================== Start Training ======================\")\n",
    "    for e in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        train_count = len(train_loader.dataset)\n",
    "        val_count = len(val_loader.dataset)\n",
    "\n",
    "        # Train the model\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(output, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_labels_list = []\n",
    "        val_preds_list = []\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                val_outputs = model(data)\n",
    "                val_loss = criterion(val_outputs, labels)\n",
    "                validation_loss += val_loss.item()\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_correct += torch.sum(val_preds == labels.data)\n",
    "                \n",
    "                # Collect the true labels and predictions for the confusion matrix\n",
    "                val_labels_list.append(labels.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "                val_preds_list.append(val_preds.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "\n",
    "        # Compute average losses and accuracies\n",
    "        train_loss = train_loss / train_count\n",
    "        train_acc = train_correct.double() / train_count\n",
    "        validation_loss = validation_loss / val_count\n",
    "        val_acc = val_correct.double() / val_count\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalar('Train Loss', train_loss, e)\n",
    "        writer.add_scalar('Train Accuracy', train_acc, e)\n",
    "        writer.add_scalar('Validation Loss', validation_loss, e)\n",
    "        writer.add_scalar('Validation Accuracy', val_acc, e)\n",
    "\n",
    "        print(f\"Epoch {e+1}/{epochs}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc*100:.2f}%, \"\n",
    "              f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "        # Store values for learning curve plotting\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(validation_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Compute and plot confusion matrix after each epoch\n",
    "        val_labels = np.concatenate(val_labels_list)\n",
    "        val_preds = np.concatenate(val_preds_list)\n",
    "        cm = confusion_matrix(val_labels, val_preds, labels=np.arange(len(np.unique(val_labels))))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=train_loader.dataset.class_names)\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title(f'Epoch {e+1} Confusion Matrix')\n",
    "        # plt.savefig(f'confusion_matrix_epoch_{e+1}.png')  # Save the confusion matrix image\n",
    "        plt.close()  # Close the figure to avoid overlap in future plots\n",
    "\n",
    "        # Save the best model based on validation accuracy\n",
    "        if val_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(), 'best_checkpoint_model.pth')\n",
    "            best_accuracy = val_acc\n",
    "        # break \n",
    "        \n",
    "    # Save the final model after training\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(epochs), train_losses, label='Train Loss', color='blue')\n",
    "    plt.plot(range(epochs), val_losses, label='Validation Loss', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve - Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('learning_curve_loss.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c90085-e986-472b-9638-1966bd7556e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== Start Training ======================\n",
      "Epoch 1/100: Train Loss: 0.0651, Train Accuracy: 15.45%, Validation Loss: 0.0666, Validation Accuracy: 24.59%\n",
      "Epoch 2/100: Train Loss: 0.0569, Train Accuracy: 29.06%, Validation Loss: 0.0597, Validation Accuracy: 25.41%\n",
      "Epoch 3/100: Train Loss: 0.0517, Train Accuracy: 42.67%, Validation Loss: 0.0534, Validation Accuracy: 40.16%\n",
      "Epoch 4/100: Train Loss: 0.0468, Train Accuracy: 50.79%, Validation Loss: 0.0514, Validation Accuracy: 40.98%\n",
      "Epoch 5/100: Train Loss: 0.0453, Train Accuracy: 51.05%, Validation Loss: 0.0496, Validation Accuracy: 48.36%\n",
      "Epoch 6/100: Train Loss: 0.0442, Train Accuracy: 52.09%, Validation Loss: 0.0487, Validation Accuracy: 45.08%\n",
      "Epoch 7/100: Train Loss: 0.0419, Train Accuracy: 54.97%, Validation Loss: 0.0475, Validation Accuracy: 47.54%\n",
      "Epoch 8/100: Train Loss: 0.0386, Train Accuracy: 63.35%, Validation Loss: 0.0473, Validation Accuracy: 51.64%\n",
      "Epoch 9/100: Train Loss: 0.0382, Train Accuracy: 61.52%, Validation Loss: 0.0443, Validation Accuracy: 52.46%\n",
      "Epoch 10/100: Train Loss: 0.0366, Train Accuracy: 62.57%, Validation Loss: 0.0433, Validation Accuracy: 55.74%\n",
      "Epoch 11/100: Train Loss: 0.0351, Train Accuracy: 66.75%, Validation Loss: 0.0422, Validation Accuracy: 54.92%\n",
      "Epoch 12/100: Train Loss: 0.0327, Train Accuracy: 69.37%, Validation Loss: 0.0421, Validation Accuracy: 54.92%\n",
      "Epoch 13/100: Train Loss: 0.0323, Train Accuracy: 69.37%, Validation Loss: 0.0410, Validation Accuracy: 57.38%\n",
      "Epoch 14/100: Train Loss: 0.0308, Train Accuracy: 72.51%, Validation Loss: 0.0400, Validation Accuracy: 57.38%\n",
      "Epoch 15/100: Train Loss: 0.0305, Train Accuracy: 72.25%, Validation Loss: 0.0396, Validation Accuracy: 62.30%\n",
      "Epoch 16/100: Train Loss: 0.0298, Train Accuracy: 71.99%, Validation Loss: 0.0426, Validation Accuracy: 49.18%\n",
      "Epoch 17/100: Train Loss: 0.0300, Train Accuracy: 70.42%, Validation Loss: 0.0400, Validation Accuracy: 59.02%\n",
      "Epoch 18/100: Train Loss: 0.0301, Train Accuracy: 69.90%, Validation Loss: 0.0397, Validation Accuracy: 62.30%\n",
      "Epoch 19/100: Train Loss: 0.0292, Train Accuracy: 73.04%, Validation Loss: 0.0383, Validation Accuracy: 57.38%\n",
      "Epoch 20/100: Train Loss: 0.0268, Train Accuracy: 76.44%, Validation Loss: 0.0393, Validation Accuracy: 57.38%\n",
      "Epoch 21/100: Train Loss: 0.0265, Train Accuracy: 75.92%, Validation Loss: 0.0385, Validation Accuracy: 61.48%\n",
      "Epoch 22/100: Train Loss: 0.0265, Train Accuracy: 75.65%, Validation Loss: 0.0372, Validation Accuracy: 63.93%\n",
      "Epoch 23/100: Train Loss: 0.0250, Train Accuracy: 80.89%, Validation Loss: 0.0383, Validation Accuracy: 59.84%\n",
      "Epoch 24/100: Train Loss: 0.0252, Train Accuracy: 78.27%, Validation Loss: 0.0381, Validation Accuracy: 63.93%\n",
      "Epoch 25/100: Train Loss: 0.0232, Train Accuracy: 79.58%, Validation Loss: 0.0369, Validation Accuracy: 60.66%\n",
      "Epoch 26/100: Train Loss: 0.0254, Train Accuracy: 75.39%, Validation Loss: 0.0387, Validation Accuracy: 63.11%\n",
      "Epoch 27/100: Train Loss: 0.0239, Train Accuracy: 79.06%, Validation Loss: 0.0389, Validation Accuracy: 62.30%\n",
      "Epoch 28/100: Train Loss: 0.0233, Train Accuracy: 79.84%, Validation Loss: 0.0360, Validation Accuracy: 62.30%\n",
      "Epoch 29/100: Train Loss: 0.0225, Train Accuracy: 78.27%, Validation Loss: 0.0352, Validation Accuracy: 69.67%\n",
      "Epoch 30/100: Train Loss: 0.0223, Train Accuracy: 81.15%, Validation Loss: 0.0379, Validation Accuracy: 59.84%\n",
      "Epoch 31/100: Train Loss: 0.0224, Train Accuracy: 80.10%, Validation Loss: 0.0349, Validation Accuracy: 68.03%\n",
      "Epoch 32/100: Train Loss: 0.0198, Train Accuracy: 83.77%, Validation Loss: 0.0347, Validation Accuracy: 68.85%\n",
      "Epoch 33/100: Train Loss: 0.0201, Train Accuracy: 86.91%, Validation Loss: 0.0354, Validation Accuracy: 63.93%\n",
      "Epoch 34/100: Train Loss: 0.0199, Train Accuracy: 84.55%, Validation Loss: 0.0346, Validation Accuracy: 68.85%\n",
      "Epoch 35/100: Train Loss: 0.0205, Train Accuracy: 82.72%, Validation Loss: 0.0345, Validation Accuracy: 68.85%\n",
      "Epoch 36/100: Train Loss: 0.0185, Train Accuracy: 87.96%, Validation Loss: 0.0342, Validation Accuracy: 68.03%\n",
      "Epoch 37/100: Train Loss: 0.0193, Train Accuracy: 84.82%, Validation Loss: 0.0347, Validation Accuracy: 68.85%\n",
      "Epoch 38/100: Train Loss: 0.0187, Train Accuracy: 84.55%, Validation Loss: 0.0336, Validation Accuracy: 69.67%\n",
      "Epoch 39/100: Train Loss: 0.0176, Train Accuracy: 87.96%, Validation Loss: 0.0346, Validation Accuracy: 67.21%\n",
      "Epoch 40/100: Train Loss: 0.0171, Train Accuracy: 86.65%, Validation Loss: 0.0328, Validation Accuracy: 68.85%\n",
      "Epoch 41/100: Train Loss: 0.0177, Train Accuracy: 87.43%, Validation Loss: 0.0340, Validation Accuracy: 69.67%\n",
      "Epoch 42/100: Train Loss: 0.0174, Train Accuracy: 87.70%, Validation Loss: 0.0326, Validation Accuracy: 72.13%\n",
      "Epoch 43/100: Train Loss: 0.0186, Train Accuracy: 85.60%, Validation Loss: 0.0353, Validation Accuracy: 67.21%\n",
      "Epoch 44/100: Train Loss: 0.0167, Train Accuracy: 87.43%, Validation Loss: 0.0328, Validation Accuracy: 68.85%\n",
      "Epoch 45/100: Train Loss: 0.0171, Train Accuracy: 87.43%, Validation Loss: 0.0348, Validation Accuracy: 72.95%\n",
      "Epoch 46/100: Train Loss: 0.0161, Train Accuracy: 87.96%, Validation Loss: 0.0335, Validation Accuracy: 68.03%\n",
      "Epoch 47/100: Train Loss: 0.0175, Train Accuracy: 86.91%, Validation Loss: 0.0319, Validation Accuracy: 70.49%\n",
      "Epoch 48/100: Train Loss: 0.0164, Train Accuracy: 88.48%, Validation Loss: 0.0337, Validation Accuracy: 67.21%\n",
      "Epoch 49/100: Train Loss: 0.0153, Train Accuracy: 90.58%, Validation Loss: 0.0339, Validation Accuracy: 71.31%\n",
      "Epoch 50/100: Train Loss: 0.0170, Train Accuracy: 88.48%, Validation Loss: 0.0323, Validation Accuracy: 67.21%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "train(num_epochs, train_loader, test_loader, criterion, optimizer, device, writer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f204a-d578-4253-b047-6a44a6dc1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            test_path (str): Path to the directory containing images.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.test_path = test_path\n",
    "        self.transform = transform\n",
    "        # Get the list of image filenames in the directory (considering jpg, jpeg, png)\n",
    "        self.image_files = [f for f in os.listdir(test_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        # print(self.image_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of images in the directory\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_name = os.path.join(self.test_path, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure it's in RGB mode\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return the image and a placeholder for labels (None or empty tuple)\n",
    "        return image\n",
    "\n",
    "# Example usage:\n",
    "# Apply the necessary transformations (e.g., resizing and normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Assuming the test images are stored in the 'Dataset/ChildData/Croped/Test/' folder\n",
    "test_dataset = TestDataset(test_path='Dataset/ChildData/TIF_DB/', transform=transform)\n",
    "\n",
    "# You can now use a DataLoader to load the test dataset\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=131, shuffle=False)\n",
    "\n",
    "# The test_loader can now be used to evaluate your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5d410-3d9f-4c36-b46f-bfc68ff2bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f25b45-3014-40ad-a12f-e38855618c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_loader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02676bca-cb45-49c3-b900-2445d68a721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "def evaluate_and_print_results(test_loader, model, device, class_names):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Iterate through the test_loader to run inference\n",
    "    with torch.no_grad():  # No need to calculate gradients for inference\n",
    "        for data in test_loader:  # Ignore labels (which are None)\n",
    "            # Get the image filenames from the test dataset\n",
    "            image_filenames = test_loader.dataset.image_files\n",
    "\n",
    "            data = data.to(device)  # Move data to the appropriate device (e.g., 'cuda' or 'cpu')\n",
    "            outputs = model(data)  # Forward pass to get predictions\n",
    "\n",
    "            # Get the predicted class for each image in the batch\n",
    "            _, predicted_classes = torch.max(outputs, 1)\n",
    "\n",
    "            # Print out the image filenames and the corresponding predicted classes\n",
    "            for idx, filename in enumerate(image_filenames):\n",
    "                predicted_class = predicted_classes[idx].item()  # Get the predicted class index\n",
    "                print(f\"Image: {filename}, Predicted Class: {class_names[predicted_class]}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a test_loader, a trained model, and the class names\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sad', 'Surprised']  # Example class names\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Make sure to use the appropriate device\n",
    "\n",
    "# Run the function to print results\n",
    "evaluate_and_print_results(test_loader, model, device, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018dfb4f-ed6c-4d93-b0a4-41ef76c74cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRAD CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913462a5-03e8-4770-b8c7-8d3fbee242cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            test_path (str): Path to the directory containing subfolders of images.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.test_path = test_path\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get the list of image filenames and labels from subfolders\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        self.class_names = os.listdir(test_path)  # Subfolder names represent the class labels\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.class_names)}\n",
    "        \n",
    "        for class_name in self.class_names:\n",
    "            class_folder = os.path.join(test_path, class_name)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for img_name in os.listdir(class_folder):\n",
    "                    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.image_files.append(os.path.join(class_folder, img_name))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_name = self.image_files[idx]\n",
    "        image = Image.open(img_name).convert('RGB')  # Ensure it's in RGB mode\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Apply the necessary transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Assuming the test images are stored in the 'Dataset/ChildData/OUTPUTFOLDER/' folder\n",
    "test_dataset = TestDataset(test_path='Dataset/ChildData/Croped/Test/', transform=transform)\n",
    "\n",
    "# Create DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=131, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525a4c6-a57d-457f-9eee-58c4ba5980ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39d8e9-f990-4b2a-8c27-01c995ea96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a523a17-a8ab-4d87-8cb7-c0caf512a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and calculate accuracy and confusion matrix\n",
    "def evaluate_and_print_results(test_loader, model, device, class_names):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Iterate through the test_loader to run inference\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device)  # Move data to the appropriate device (e.g., 'cuda' or 'cpu')\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(data)  # Forward pass to get predictions\n",
    "            _, predicted_classes = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(predicted_classes.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize by row (true label)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sad', 'Surprised']  # Example class names\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, else CPU\n",
    "\n",
    "model.load_state_dict(torch.load('final_model.pth'))\n",
    "# Run the function to print results\n",
    "evaluate_and_print_results(test_loader, model, device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d496e9a-d42b-4bc9-80a6-1a0aecb5b52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
