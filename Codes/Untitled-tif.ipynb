{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d89d822-fba4-41ed-8908-10c42a531d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n",
      "['Fear', 'Sad', 'Angry', 'Neutral', 'Surprised', 'Happiness', 'Disgust']\n",
      "Training Set: Fear = 5\n",
      "Testing Set: Fear = 2\n",
      "--------------------\n",
      "Training Set: Sad = 21\n",
      "Testing Set: Sad = 6\n",
      "--------------------\n",
      "Training Set: Angry = 6\n",
      "Testing Set: Angry = 2\n",
      "--------------------\n",
      "Training Set: Neutral = 22\n",
      "Testing Set: Neutral = 6\n",
      "--------------------\n",
      "Training Set: Surprised = 10\n",
      "Testing Set: Surprised = 3\n",
      "--------------------\n",
      "Training Set: Happiness = 26\n",
      "Testing Set: Happiness = 7\n",
      "--------------------\n",
      "Training Set: Disgust = 11\n",
      "Testing Set: Disgust = 3\n",
      "--------------------\n",
      "Using device: cuda:2\n",
      "Train dataset size: 101, Test dataset size: 29\n",
      "Using batch size: 10\n",
      "Using device: cuda:2\n",
      "Model is on device: cuda:2\n",
      "Total parameters: 38,263,318\n",
      "Trainable parameters: 38,263,318\n",
      "Percentage of trainable parameters: 100.00%\n",
      "Initial validation metrics: {'val_triplet_loss': 0.49245306849479675, 'val_class_loss': 1.9555381536483765, 'val_class_acc': 0.06666667014360428, 'val_triplet_acc': 0.4851852059364319}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                           \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 2 has a total capacity of 31.74 GiB of which 12.69 MiB is free. Process 35912 has 2.19 GiB memory in use. Process 36921 has 12.30 GiB memory in use. Process 38269 has 12.30 GiB memory in use. Process 49199 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 4.29 GiB memory in use. Of the allocated memory 3.08 GiB is allocated by PyTorch, and 28.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1020\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;66;03m# This is the key to fixing the multiprocessing error\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m     multiprocessing\u001b[38;5;241m.\u001b[39mfreeze_support()\n\u001b[0;32m-> 1020\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 858\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinuing with training anyway...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfit_one_cycle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased epochs for triplet learning\u001b[39;49;00m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# Plot training metrics\u001b[39;00m\n\u001b[1;32m    870\u001b[0m plot_metrics(history)\n",
      "Cell \u001b[0;32mIn[5], line 575\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(epochs, max_lr, model, train_loader, val_loader, weight_decay, grad_clip, opt_func)\u001b[0m\n\u001b[1;32m    572\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[0;32m--> 575\u001b[0m     total_loss, triplet_loss, class_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m     train_total_losses\u001b[38;5;241m.\u001b[39mappend(total_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    578\u001b[0m     train_triplet_losses\u001b[38;5;241m.\u001b[39mappend(triplet_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[5], line 244\u001b[0m, in \u001b[0;36mMultiTaskTripletBase.training_step\u001b[0;34m(self, batch, alpha)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Get embeddings and class predictions\u001b[39;00m\n\u001b[1;32m    243\u001b[0m anchor_embed, anchor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(anchor)\n\u001b[0;32m--> 244\u001b[0m positive_embed, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m negative_embed, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(negative)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Calculate losses\u001b[39;00m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 432\u001b[0m, in \u001b[0;36mEnsembleTripletNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# Extract features from all backbones\u001b[39;00m\n\u001b[1;32m    431\u001b[0m     feat1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone1(x)\n\u001b[0;32m--> 432\u001b[0m     feat2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     feat3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone3(x)\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Apply attention if features are 4D\u001b[39;00m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/timm/models/efficientnet.py:268\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/timm/models/efficientnet.py:252\u001b[0m, in \u001b[0;36mEfficientNet.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_stem(x)\n\u001b[0;32m--> 252\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    254\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x, flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/timm/layers/norm_act.py:127\u001b[0m, in \u001b[0;36mBatchNormAct2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    116\u001b[0m     x,\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[0;32m--> 127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:405\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2104\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 2104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 2 has a total capacity of 31.74 GiB of which 12.69 MiB is free. Process 35912 has 2.19 GiB memory in use. Process 36921 has 12.30 GiB memory in use. Process 38269 has 12.30 GiB memory in use. Process 49199 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 4.29 GiB memory in use. Of the allocated memory 3.08 GiB is allocated by PyTorch, and 28.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import random\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import timm\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "\n",
    "# Dataset paths\n",
    "dataset = 'TIF_DF'\n",
    "print(os.listdir(dataset))\n",
    "\n",
    "classes = os.listdir(dataset + \"/train\")\n",
    "print(classes)\n",
    "\n",
    "data_dir = dataset\n",
    "train_dir = data_dir + '/train'\n",
    "test_dir = data_dir + '/test'\n",
    "\n",
    "# Print class distribution\n",
    "count = []\n",
    "for folder in classes:\n",
    "    num_images_train = len(os.listdir(train_dir + '/' + folder))\n",
    "    num_images_test = len(os.listdir(test_dir + '/' + folder))\n",
    "    count.append(num_images_train)\n",
    "    print(f'Training Set: {folder} = {num_images_train}')\n",
    "    print(f'Testing Set: {folder} = {num_images_test}')\n",
    "    print(\"--\" * 10)\n",
    "\n",
    "\n",
    "# Define device (GPU or CPU)\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda:2')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            # Handle triplet batches (anchor, positive, negative, label, metadata)\n",
    "            if isinstance(b, tuple) and len(b) == 5:\n",
    "                anchor, positive, negative, labels, metadata = b\n",
    "                # Make sure to return as a tuple, not a list\n",
    "                yield (to_device(anchor, self.device), \n",
    "                       to_device(positive, self.device), \n",
    "                       to_device(negative, self.device), \n",
    "                       to_device(labels, self.device), \n",
    "                       metadata)\n",
    "            else:\n",
    "                yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Image transformations with data augmentation\n",
    "stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet stats for normalization\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Larger size for better feature extraction\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])\n",
    "\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Create a dictionary of images for each class\n",
    "        self.class_to_images = {i: [] for i in range(len(self.classes))}\n",
    "        self.images = []\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(root_dir, cls)\n",
    "            class_idx = self.class_to_idx[cls]\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                self.images.append((img_path, class_idx, cls))\n",
    "                self.class_to_images[class_idx].append((img_path, cls))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_path, anchor_class, anchor_class_name = self.images[idx]\n",
    "        \n",
    "        # Get positive example (same class as anchor)\n",
    "        positive_images = [img for img in self.class_to_images[anchor_class] if img[0] != anchor_path]\n",
    "        if positive_images:\n",
    "            positive_path, positive_class_name = random.choice(positive_images)\n",
    "        else:\n",
    "            # If no other images in the same class, use the same image\n",
    "            positive_path, positive_class_name = anchor_path, anchor_class_name\n",
    "        \n",
    "        # Get negative example (different class from anchor)\n",
    "        negative_class = random.choice([c for c in range(len(self.classes)) if c != anchor_class])\n",
    "        negative_path, negative_class_name = random.choice(self.class_to_images[negative_class])\n",
    "        \n",
    "        # Load images\n",
    "        anchor_img = Image.open(anchor_path).convert('RGB')\n",
    "        positive_img = Image.open(positive_path).convert('RGB')\n",
    "        negative_img = Image.open(negative_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "        \n",
    "        # Create labels tensor for multi-task learning\n",
    "        labels = torch.tensor(anchor_class, dtype=torch.long)\n",
    "        \n",
    "        # Return metadata for visualization\n",
    "        metadata = {\n",
    "            'anchor_class': anchor_class_name,\n",
    "            'positive_class': positive_class_name,\n",
    "            'negative_class': negative_class_name,\n",
    "            'anchor_path': anchor_path,\n",
    "            'positive_path': positive_path,\n",
    "            'negative_path': negative_path\n",
    "        }\n",
    "        \n",
    "        return anchor_img, positive_img, negative_img, labels, metadata\n",
    "\n",
    "\n",
    "class TripletDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, num_workers=0, **kwargs):\n",
    "        super(TripletDataLoader, self).__init__(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "            num_workers=num_workers, collate_fn=self.collate_fn, **kwargs\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        anchors = []\n",
    "        positives = []\n",
    "        negatives = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        for anchor, positive, negative, label, meta in batch:\n",
    "            anchors.append(anchor)\n",
    "            positives.append(positive)\n",
    "            negatives.append(negative)\n",
    "            labels.append(label)\n",
    "            metadata.append(meta)\n",
    "        \n",
    "        return torch.stack(anchors), torch.stack(positives), torch.stack(negatives), \\\n",
    "               torch.stack(labels), metadata\n",
    "\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // 16, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // 16, in_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_att = self.channel_attention(x)\n",
    "        x_channel = x * channel_att\n",
    "        \n",
    "        spatial_att = self.spatial_attention(x_channel)\n",
    "        x_out = x_channel * spatial_att\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "\n",
    "class MultiTaskTripletBase(nn.Module):\n",
    "    def calculate_triplet_loss(self, anchor, positive, negative, margin=0.5):\n",
    "        distance_positive = F.pairwise_distance(anchor, positive)\n",
    "        distance_negative = F.pairwise_distance(anchor, negative)\n",
    "        losses = F.relu(distance_positive - distance_negative + margin)\n",
    "        return losses.mean()\n",
    "    \n",
    "    def calculate_classification_loss(self, class_preds, class_labels):\n",
    "        return F.cross_entropy(class_preds, class_labels)\n",
    "    \n",
    "    def training_step(self, batch, alpha=0.7):\n",
    "        anchor, positive, negative, class_labels, _ = batch\n",
    "        \n",
    "        # Get embeddings and class predictions\n",
    "        anchor_embed, anchor_class = self(anchor)\n",
    "        positive_embed, _ = self(positive)\n",
    "        negative_embed, _ = self(negative)\n",
    "        \n",
    "        # Calculate losses\n",
    "        triplet_loss = self.calculate_triplet_loss(anchor_embed, positive_embed, negative_embed)\n",
    "        classification_loss = self.calculate_classification_loss(anchor_class, class_labels)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = alpha * triplet_loss + (1 - alpha) * classification_loss\n",
    "        \n",
    "        return total_loss, triplet_loss, classification_loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        anchor, positive, negative, class_labels, _ = batch\n",
    "        \n",
    "        # Get embeddings and class predictions\n",
    "        anchor_embed, anchor_class = self(anchor)\n",
    "        positive_embed, _ = self(positive)\n",
    "        negative_embed, _ = self(negative)\n",
    "        \n",
    "        # Calculate losses\n",
    "        triplet_loss = self.calculate_triplet_loss(anchor_embed, positive_embed, negative_embed)\n",
    "        classification_loss = self.calculate_classification_loss(anchor_class, class_labels)\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        _, predicted = anchor_class.max(1)\n",
    "        correct = predicted.eq(class_labels).sum().item()\n",
    "        class_acc = correct / len(class_labels)\n",
    "        \n",
    "        # Calculate triplet accuracy (positive closer than negative)\n",
    "        dist_pos = F.pairwise_distance(anchor_embed, positive_embed)\n",
    "        dist_neg = F.pairwise_distance(anchor_embed, negative_embed)\n",
    "        triplet_correct = (dist_pos < dist_neg).sum().item()\n",
    "        triplet_acc = triplet_correct / len(dist_pos)\n",
    "        \n",
    "        return {\n",
    "            'val_triplet_loss': triplet_loss.detach(),\n",
    "            'val_class_loss': classification_loss.detach(),\n",
    "            'val_class_acc': torch.tensor(class_acc, device=triplet_loss.device),\n",
    "            'val_triplet_acc': torch.tensor(triplet_acc, device=triplet_loss.device)\n",
    "        }\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_triplet_losses = [x['val_triplet_loss'] for x in outputs]\n",
    "        batch_class_losses = [x['val_class_loss'] for x in outputs]\n",
    "        batch_class_accs = [x['val_class_acc'] for x in outputs]\n",
    "        batch_triplet_accs = [x['val_triplet_acc'] for x in outputs]\n",
    "        \n",
    "        epoch_triplet_loss = torch.stack(batch_triplet_losses).mean()\n",
    "        epoch_class_loss = torch.stack(batch_class_losses).mean()\n",
    "        epoch_class_acc = torch.stack(batch_class_accs).mean()\n",
    "        epoch_triplet_acc = torch.stack(batch_triplet_accs).mean()\n",
    "        \n",
    "        return {\n",
    "            'val_triplet_loss': epoch_triplet_loss.item(),\n",
    "            'val_class_loss': epoch_class_loss.item(),\n",
    "            'val_class_acc': epoch_class_acc.item(),\n",
    "            'val_triplet_acc': epoch_triplet_acc.item()\n",
    "        }\n",
    "    \n",
    "    def epoch_end(self, epoch, result, train_losses=None):\n",
    "        if train_losses:\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"  Train total_loss: {train_losses['total']:.4f}, \"\n",
    "                  f\"triplet_loss: {train_losses['triplet']:.4f}, \"\n",
    "                  f\"class_loss: {train_losses['class']:.4f}\")\n",
    "        \n",
    "        print(f\"  Val triplet_loss: {result['val_triplet_loss']:.4f}, \"\n",
    "              f\"class_loss: {result['val_class_loss']:.4f}, \"\n",
    "              f\"class_acc: {result['val_class_acc']:.4f}, \"\n",
    "              f\"triplet_acc: {result['val_triplet_acc']:.4f}\")\n",
    "\n",
    "\n",
    "class EnhancedTripletNetwork(MultiTaskTripletBase):\n",
    "    def __init__(self, num_classes, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use face-specific pre-trained model (ResNet50 with IR blocks)\n",
    "        self.face_backbone = timm.create_model('resnet50', pretrained=True)\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        self.feature_dim = self.face_backbone.fc.in_features\n",
    "        self.face_backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Attention modules\n",
    "        self.attention = AttentionModule(2048)\n",
    "        \n",
    "        # Embedding network with attention\n",
    "        self.embedding_network = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Classification head for multi-task learning\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Freeze early layers for transfer learning\n",
    "        for name, param in self.face_backbone.named_parameters():\n",
    "            if 'layer4' not in name and 'layer3' not in name:\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        \"\"\"Forward pass for one input with attention\"\"\"\n",
    "        # Extract features from backbone\n",
    "        features = self.face_backbone(x)\n",
    "        \n",
    "        # If features are 4D (B, C, H, W), apply attention\n",
    "        if len(features.shape) == 4:\n",
    "            features = self.attention(features)\n",
    "            features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for the triplet network with multi-task learning\"\"\"\n",
    "        # Extract features\n",
    "        features = self.forward_one(x)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embeddings = self.embedding_network(features)\n",
    "        \n",
    "        # Get class predictions\n",
    "        class_predictions = self.classifier(embeddings)\n",
    "        \n",
    "        return embeddings, class_predictions\n",
    "\n",
    "\n",
    "class EnsembleTripletNetwork(MultiTaskTripletBase):\n",
    "    def __init__(self, num_classes, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multiple backbones for ensemble\n",
    "        self.backbone1 = timm.create_model('resnet50', pretrained=True)\n",
    "        self.backbone2 = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        self.backbone3 = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
    "        \n",
    "        # Get feature dimensions\n",
    "        self.dim1 = self.backbone1.fc.in_features\n",
    "        self.dim2 = self.backbone2.classifier.in_features\n",
    "        self.dim3 = self.backbone3.classifier.in_features\n",
    "        \n",
    "        # Remove final layers\n",
    "        self.backbone1.fc = nn.Identity()\n",
    "        self.backbone2.classifier = nn.Identity()\n",
    "        self.backbone3.classifier = nn.Identity()\n",
    "        \n",
    "        # Attention modules for each backbone\n",
    "        self.attention1 = AttentionModule(self.dim1)\n",
    "        self.attention2 = AttentionModule(self.dim2)\n",
    "        self.attention3 = AttentionModule(self.dim3)\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.dim1 + self.dim2 + self.dim3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Embedding network\n",
    "        self.embedding_network = nn.Sequential(\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features from all backbones\n",
    "        feat1 = self.backbone1(x)\n",
    "        feat2 = self.backbone2(x)\n",
    "        feat3 = self.backbone3(x)\n",
    "        \n",
    "        # Apply attention if features are 4D\n",
    "        if len(feat1.shape) == 4:\n",
    "            feat1 = self.attention1(feat1)\n",
    "            feat1 = F.adaptive_avg_pool2d(feat1, (1, 1))\n",
    "            feat1 = feat1.view(feat1.size(0), -1)\n",
    "        \n",
    "        if len(feat2.shape) == 4:\n",
    "            feat2 = self.attention2(feat2)\n",
    "            feat2 = F.adaptive_avg_pool2d(feat2, (1, 1))\n",
    "            feat2 = feat2.view(feat2.size(0), -1)\n",
    "        \n",
    "        if len(feat3.shape) == 4:\n",
    "            feat3 = self.attention3(feat3)\n",
    "            feat3 = F.adaptive_avg_pool2d(feat3, (1, 1))\n",
    "            feat3 = feat3.view(feat3.size(0), -1)\n",
    "        \n",
    "        # Concatenate features\n",
    "        fused_features = torch.cat([feat1, feat2, feat3], dim=1)\n",
    "        fused_features = self.fusion(fused_features)\n",
    "        \n",
    "        # Get embeddings and predictions\n",
    "        embeddings = self.embedding_network(fused_features)\n",
    "        class_predictions = self.classifier(embeddings)\n",
    "        \n",
    "        return embeddings, class_predictions\n",
    "\n",
    "\n",
    "# Evaluation function for validation\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        # Ensure model and batch are on the same device\n",
    "        if isinstance(batch, tuple) and len(batch) == 5:\n",
    "            anchor, positive, negative, labels, metadata = batch\n",
    "            # Double check device\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "            labels = labels.to(device)\n",
    "            batch = (anchor, positive, negative, labels, metadata)\n",
    "        \n",
    "        output = model.validation_step(batch)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "# Get learning rate from optimizer\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# Function to plot training and validation metrics\n",
    "def plot_metrics(history):\n",
    "    train_triplet_losses = [x.get('train_triplet_loss', 0) for x in history if 'train_triplet_loss' in x]\n",
    "    train_class_losses = [x.get('train_class_loss', 0) for x in history if 'train_class_loss' in x]\n",
    "    val_triplet_losses = [x['val_triplet_loss'] for x in history]\n",
    "    val_class_losses = [x['val_class_loss'] for x in history]\n",
    "    val_class_accs = [x['val_class_acc'] for x in history]\n",
    "    val_triplet_accs = [x['val_triplet_acc'] for x in history]\n",
    "\n",
    "    # Create figure with 2x2 subplots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot triplet loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(train_triplet_losses, label='Train Triplet Loss')\n",
    "    plt.plot(val_triplet_losses, label='Val Triplet Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Triplet Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot classification loss\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(train_class_losses, label='Train Class Loss')\n",
    "    plt.plot(val_class_losses, label='Val Class Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Classification Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot classification accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(val_class_accs, label='Val Class Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot triplet accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(val_triplet_accs, label='Val Triplet Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Triplet Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('triplet_training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Training function with one-cycle learning rate schedule\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = opt_func(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    # For tracking learning rates\n",
    "    lrs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_total_losses = []\n",
    "        train_triplet_losses = []\n",
    "        train_class_losses = []\n",
    "        \n",
    "        # Create progress bar\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "        \n",
    "        for batch in loop:\n",
    "            total_loss, triplet_loss, class_loss = model.training_step(batch)\n",
    "            \n",
    "            train_total_losses.append(total_loss.item())\n",
    "            train_triplet_losses.append(triplet_loss.item())\n",
    "            train_class_losses.append(class_loss.item())\n",
    "            \n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Gradient clipping if specified\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            loop.set_postfix(total_loss=total_loss.item(), triplet_loss=triplet_loss.item(), \n",
    "                            class_loss=class_loss.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        \n",
    "        # Record training losses\n",
    "        result['train_total_loss'] = np.mean(train_total_losses)\n",
    "        result['train_triplet_loss'] = np.mean(train_triplet_losses)\n",
    "        result['train_class_loss'] = np.mean(train_class_losses)\n",
    "        \n",
    "        # Record learning rates for this epoch\n",
    "        result['lrs'] = lrs\n",
    "        \n",
    "        # Print epoch results\n",
    "        model.epoch_end(epoch, result, {\n",
    "            'total': result['train_total_loss'],\n",
    "            'triplet': result['train_triplet_loss'],\n",
    "            'class': result['train_class_loss']\n",
    "        })\n",
    "        \n",
    "        # Save history\n",
    "        history.append(result)\n",
    "        \n",
    "        # Save model after each epoch\n",
    "        torch.save(model.state_dict(), 'latest_triplet_model.pth')\n",
    "        \n",
    "        # Save best model if validation metrics improve\n",
    "        if epoch == 0 or result['val_class_acc'] > max([h['val_class_acc'] for h in history[:-1]]):\n",
    "            torch.save(model.state_dict(), 'best_triplet_model.pth')\n",
    "            print(f\"Model saved at epoch {epoch} with val_class_acc: {result['val_class_acc']:.4f}\")\n",
    "    \n",
    "    return history\n",
    "def evaluate_triplet_test_set(model, test_dl, classes):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_class_preds = []\n",
    "    all_class_labels = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl, desc=\"Evaluating test set\"):\n",
    "            anchor, positive, negative, class_labels, metadata = batch\n",
    "            anchor_embed, anchor_class = model(anchor)\n",
    "            \n",
    "            all_embeddings.append(anchor_embed.cpu().numpy())\n",
    "            all_class_preds.append(anchor_class.cpu().numpy())\n",
    "            all_class_labels.append(class_labels.cpu().numpy())\n",
    "            all_metadata.extend(metadata)\n",
    "    \n",
    "    all_embeddings = np.concatenate(all_embeddings)\n",
    "    all_class_preds = np.concatenate(all_class_preds)\n",
    "    all_class_labels = np.concatenate(all_class_labels)\n",
    "    \n",
    "    # Save embeddings and predictions for later analysis if needed\n",
    "    np.save('all_embeddings.npy', all_embeddings)\n",
    "    np.save('all_class_preds.npy', all_class_preds)\n",
    "    np.save('all_class_labels.npy', all_class_labels)\n",
    "    \n",
    "    # Get predicted classes\n",
    "    all_predicted_classes = np.argmax(all_class_preds, axis=1)\n",
    "    \n",
    "    # Calculate classification accuracy\n",
    "    class_acc = accuracy_score(all_class_labels, all_predicted_classes)\n",
    "    print(f'Test Classification Accuracy: {class_acc * 100:.2f}%')\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_class_labels, all_predicted_classes, target_names=classes))\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_class_labels, all_predicted_classes)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('triplet_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create embedding visualization using t-SNE with error handling\n",
    "    try:\n",
    "        from sklearn.manifold import TSNE\n",
    "        \n",
    "        # Calculate appropriate perplexity (should be smaller than n_samples)\n",
    "        n_samples = len(all_embeddings)\n",
    "        perplexity = min(30, max(5, n_samples // 4))  # Use appropriate perplexity\n",
    "        \n",
    "        print(f\"Running t-SNE with {n_samples} samples and perplexity={perplexity}\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "        embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        for i, class_name in enumerate(classes):\n",
    "            mask = all_class_labels == i\n",
    "            plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "                       label=class_name, alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('t-SNE Component 1')\n",
    "        plt.ylabel('t-SNE Component 2')\n",
    "        plt.title('t-SNE Visualization of Embeddings')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('triplet_embeddings_tsne.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating t-SNE visualization: {e}\")\n",
    "        print(\"Skipping t-SNE visualization.\")\n",
    "    \n",
    "    return class_acc, cm\n",
    "\n",
    "# Function to visualize triplet predictions\n",
    "def visualize_triplet_predictions(model, test_dl, classes, num_samples=5):\n",
    "    # Get a batch of triplets\n",
    "    dataiter = iter(test_dl)\n",
    "    anchor, positive, negative, class_labels, metadata = next(dataiter)\n",
    "    \n",
    "    # Get embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        anchor_embed, anchor_class = model(anchor)\n",
    "        positive_embed, _ = model(positive)\n",
    "        negative_embed, _ = model(negative)\n",
    "    \n",
    "    # Convert to CPU\n",
    "    anchor = anchor.cpu()\n",
    "    positive = positive.cpu()\n",
    "    negative = negative.cpu()\n",
    "    anchor_embed = anchor_embed.cpu()\n",
    "    positive_embed = positive_embed.cpu()\n",
    "    negative_embed = negative_embed.cpu()\n",
    "    anchor_class = anchor_class.cpu()\n",
    "    \n",
    "    # Function to denormalize images for display\n",
    "    def denormalize(image, mean=stats[0], std=stats[1]):\n",
    "        img_denorm = image.clone()\n",
    "        for i in range(3):\n",
    "            img_denorm[i] = img_denorm[i] * std[i] + mean[i]\n",
    "        return torch.clamp(img_denorm, 0, 1)\n",
    "    \n",
    "    # Plot the triplets with distances\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i in range(min(num_samples, len(anchor))):\n",
    "        # Denormalize images\n",
    "        anchor_display = denormalize(anchor[i])\n",
    "        positive_display = denormalize(positive[i])\n",
    "        negative_display = denormalize(negative[i])\n",
    "        \n",
    "        # Convert to numpy for matplotlib\n",
    "        anchor_display = anchor_display.permute(1, 2, 0).numpy()\n",
    "        positive_display = positive_display.permute(1, 2, 0).numpy()\n",
    "        negative_display = negative_display.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Plot anchor\n",
    "        plt.subplot(3, num_samples, i + 1)\n",
    "        plt.imshow(anchor_display)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Anchor: {metadata[i]['anchor_class']}\")\n",
    "        \n",
    "        # Plot positive\n",
    "        plt.subplot(3, num_samples, i + 1 + num_samples)\n",
    "        plt.imshow(positive_display)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Positive: {metadata[i]['positive_class']}\")\n",
    "        \n",
    "        # Plot negative\n",
    "        plt.subplot(3, num_samples, i + 1 + 2*num_samples)\n",
    "        plt.imshow(negative_display)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Negative: {metadata[i]['negative_class']}\")\n",
    "        \n",
    "        # Calculate distances\n",
    "        pos_dist = F.pairwise_distance(anchor_embed[i:i+1], positive_embed[i:i+1])\n",
    "        neg_dist = F.pairwise_distance(anchor_embed[i:i+1], negative_embed[i:i+1])\n",
    "        \n",
    "        # Get class prediction\n",
    "        _, pred_class = anchor_class[i].max(0)\n",
    "        pred_class_name = classes[pred_class.item()]\n",
    "        \n",
    "        # Add distance information\n",
    "        color = 'green' if pos_dist < neg_dist else 'red'\n",
    "        plt.figtext(0.1 + (i * 0.2), 0.01,\n",
    "                   f\"Pos dist: {pos_dist.item():.2f}\\n\"\n",
    "                   f\"Neg dist: {neg_dist.item():.2f}\\n\"\n",
    "                   f\"Predicted: {pred_class_name}\",\n",
    "                   color=color, ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.savefig('triplet_sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "     # Create triplet datasets\n",
    "    train_ds = TripletDataset(train_dir, train_transforms)\n",
    "    test_ds = TripletDataset(test_dir, test_transforms)\n",
    "    \n",
    "    # Determine appropriate batch size based on dataset size\n",
    "    train_size = len(train_ds)\n",
    "    test_size = len(test_ds)\n",
    "    print(f\"Train dataset size: {train_size}, Test dataset size: {test_size}\")\n",
    "    \n",
    "    # Adjust batch size based on dataset size\n",
    "    batch_size = min(32, max(1, train_size // 10))  # Ensure at least 10 batches for training\n",
    "    print(f\"Using batch size: {batch_size}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dl = TripletDataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_dl = TripletDataLoader(test_ds, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    # Get device\n",
    "    device = get_default_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move data to device - ensure this is called\n",
    "    train_dl = DeviceDataLoader(train_dl, device)\n",
    "    test_dl = DeviceDataLoader(test_dl, device)\n",
    "    \n",
    "    # Create model\n",
    "    model = EnsembleTripletNetwork(num_classes=len(classes))\n",
    "    \n",
    "    # Explicitly move model to device and ensure it's there\n",
    "    model = model.to(device)\n",
    "    print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")\n",
    "    \n",
    "    # Add this right before the initial_result = evaluate(model, test_dl) line\n",
    "    try:\n",
    "        # Get initial validation metrics\n",
    "        initial_result = evaluate(model, test_dl)\n",
    "        print(\"Initial validation metrics:\", initial_result)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during initial evaluation: {e}\")\n",
    "        print(\"Trying to fix device issues...\")\n",
    "        \n",
    "        # Force model to device again\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Try again\n",
    "        try:\n",
    "            initial_result = evaluate(model, test_dl)\n",
    "            print(\"Initial validation metrics:\", initial_result)\n",
    "        except Exception as e:\n",
    "            print(f\"Still having issues: {e}\")\n",
    "            print(\"Continuing with training anyway...\")\n",
    "    # ---------------------------\n",
    "    \n",
    "    # Train the model\n",
    "    history = fit_one_cycle(\n",
    "        epochs=25,  # Increased epochs for triplet learning\n",
    "        max_lr=0.001,\n",
    "        model=model,\n",
    "        train_loader=train_dl,\n",
    "        val_loader=test_dl,\n",
    "        weight_decay=0.001,\n",
    "        grad_clip=0.1,\n",
    "        opt_func=torch.optim.Adam\n",
    "    )\n",
    "    \n",
    "    # Plot training metrics\n",
    "    plot_metrics(history)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'final_triplet_model.pth')\n",
    "    \n",
    "    # Load the best model for evaluation\n",
    "    model.load_state_dict(torch.load('best_triplet_model.pth'))\n",
    "    model = to_device(model, device)\n",
    "    \n",
    "    # Evaluate the model on the test dataset\n",
    "    test_acc, conf_matrix = evaluate_triplet_test_set(model, test_dl, classes)\n",
    "    \n",
    "    # Visualize some predictions\n",
    "    visualize_triplet_predictions(model, test_dl, classes, num_samples=5)\n",
    "    \n",
    "    # Plot learning rate vs. loss\n",
    "    if len(history) > 0 and 'lrs' in history[0]:\n",
    "        # Extract learning rates and losses\n",
    "        epochs_lrs = []\n",
    "        epochs_losses = []\n",
    "        \n",
    "        for epoch_result in history:\n",
    "            if 'lrs' in epoch_result and 'train_total_loss' in epoch_result:\n",
    "                # We'll use the average LR for the epoch\n",
    "                avg_lr = sum(epoch_result['lrs']) / len(epoch_result['lrs'])\n",
    "                epochs_lrs.append(avg_lr)\n",
    "                epochs_losses.append(epoch_result['train_total_loss'])\n",
    "        \n",
    "        # Plot LR vs. Loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs_lrs, epochs_losses, 'o-')\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate (log scale)')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title('Learning Rate vs. Training Loss')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.savefig('triplet_lr_vs_loss.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Additional utility functions for advanced visualization and analysis\n",
    "def create_embedding_space_visualization(model, data_loader, classes, num_samples=1000):\n",
    "    \"\"\"Creates a 3D visualization of the embedding space\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if i * data_loader.batch_size > num_samples:\n",
    "                break\n",
    "            \n",
    "            anchor, _, _, class_labels, _ = batch\n",
    "            anchor_embed, _ = model(anchor)\n",
    "            \n",
    "            embeddings.append(anchor_embed.cpu().numpy())\n",
    "            labels.append(class_labels.cpu().numpy())\n",
    "    \n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # PCA for 3D visualization\n",
    "    pca = PCA(n_components=3)\n",
    "    embeddings_3d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Create 3D scatter plot\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        mask = labels == i\n",
    "        ax.scatter(embeddings_3d[mask, 0], embeddings_3d[mask, 1], embeddings_3d[mask, 2], \n",
    "                  label=class_name, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('PCA Component 1')\n",
    "    ax.set_ylabel('PCA Component 2')\n",
    "    ax.set_zlabel('PCA Component 3')\n",
    "    ax.set_title('3D Visualization of Embedding Space')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('embedding_space_3d.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def hard_triplet_mining(anchor_embeddings, positive_embeddings, negative_embeddings):\n",
    "    \"\"\"Implements hard triplet mining strategy\"\"\"\n",
    "    # Calculate pairwise distances\n",
    "    pos_distances = F.pairwise_distance(anchor_embeddings, positive_embeddings)\n",
    "    neg_distances = F.pairwise_distance(anchor_embeddings, negative_embeddings)\n",
    "    \n",
    "    # Find hard positives and hard negatives\n",
    "    hard_positives = torch.argmax(pos_distances)\n",
    "    hard_negatives = torch.argmin(neg_distances)\n",
    "    \n",
    "    return hard_positives, hard_negatives\n",
    "\n",
    "\n",
    "def analyze_model_robustness(model, test_loader, classes, noise_levels=[0.1, 0.2, 0.3]):\n",
    "    \"\"\"Analyze model robustness to input perturbations\"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    for noise_level in noise_levels:\n",
    "        accuracies = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                anchor, _, _, class_labels, _ = batch\n",
    "                \n",
    "                # Add Gaussian noise\n",
    "                noise = torch.randn_like(anchor) * noise_level\n",
    "                noisy_anchor = anchor + noise\n",
    "                \n",
    "                # Get predictions\n",
    "                _, anchor_class = model(noisy_anchor)\n",
    "                _, predicted = anchor_class.max(1)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                correct = predicted.eq(class_labels).sum().item()\n",
    "                accuracy = correct / len(class_labels)\n",
    "                accuracies.append(accuracy)\n",
    "        \n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        results[noise_level] = avg_accuracy\n",
    "    \n",
    "    # Plot robustness analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    noise_levels_plot = list(results.keys())\n",
    "    accuracies_plot = list(results.values())\n",
    "    \n",
    "    plt.plot(noise_levels_plot, accuracies_plot, 'o-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Noise Level')\n",
    "    plt.ylabel('Classification Accuracy')\n",
    "    plt.title('Model Robustness to Input Noise')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_robustness_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This is the key to fixing the multiprocessing error\n",
    "    multiprocessing.freeze_support()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cc480-d187-4a60-b838-b197ed51a7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
