{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9b8b0c-c5b8-4711-904d-6b012a24a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anura\\AppData\\Local\\Temp\\ipykernel_14644\\1954328406.py:106: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  edge_index = torch.tensor(np.nonzero(adjacency_matrix), dtype=torch.long)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (101x136 and 2x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 122\u001b[0m\n\u001b[0;32m    120\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    121\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 122\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m    123\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m    124\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m, in \u001b[0;36mGCNWithPooling.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch):\n\u001b[1;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index)\n\u001b[0;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:260\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m             edge_index \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m--> 260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (101x136 and 2x64)"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCNWithPooling(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCNWithPooling, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 256)\n",
    "        self.conv4 = GCNConv(256, 128)\n",
    "        self.conv5 = GCNConv(128, 64)\n",
    "\n",
    "        self.pool1 = global_mean_pool\n",
    "        self.pool2 = global_mean_pool\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(64, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.pool1(x, batch)\n",
    "        x = self.pool2(x, batch)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "csv_path = 'tif_df_train.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df['Landmarks'] = df['Landmarks'].apply(ast.literal_eval)\n",
    "\n",
    "emotion_list = [\"Angry\", \"Disgust\", \"Fear\", \"Happiness\", \"Neutral\", \"Sad\", \"Surprised\"]\n",
    "\n",
    "def one_hot_encode(emotion, emotions=emotion_list):\n",
    "    emotion_dict = {emotion: idx for idx, emotion in enumerate(emotions)}\n",
    "    one_hot_vector = [0.0] * len(emotions)\n",
    "    if emotion in emotion_dict:\n",
    "        one_hot_vector[emotion_dict[emotion]] = 1.0\n",
    "    return one_hot_vector\n",
    "\n",
    "df['encoded_label'] = df['Labels'].apply(lambda x: one_hot_encode(x))\n",
    "df['encoded_label'] = df['encoded_label'].apply(lambda x: torch.tensor(x, dtype=torch.float32))\n",
    "\n",
    "labels = torch.stack([tensor for tensor in df['encoded_label']])\n",
    "\n",
    "landmarks = df['Landmarks'].apply(lambda x: np.array(x))\n",
    "landmarks_normalized = np.array([np.array(x) for x in landmarks])\n",
    "landmarks_reshaped = landmarks_normalized.reshape(-1, 2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "landmarks_normalized = scaler.fit_transform(landmarks_reshaped)\n",
    "landmarks_normalized = landmarks_normalized.reshape(-1, 68, 2)\n",
    "\n",
    "node_features = torch.tensor(landmarks_normalized, dtype=torch.float32)\n",
    "\n",
    "def delaunay_triangulation(landmarks):\n",
    "    from scipy.spatial import Delaunay\n",
    "    tri = Delaunay(landmarks)\n",
    "    triangles = tri.simplices\n",
    "    edges = set()\n",
    "    for triangle in triangles:\n",
    "        for i in range(3):\n",
    "            edge = tuple(sorted([triangle[i], triangle[(i + 1) % 3]]))\n",
    "            edges.add(edge)\n",
    "    return triangles, list(edges)\n",
    "\n",
    "def construct_adjacency_matrix(landmarks):\n",
    "    _, edges = delaunay_triangulation(landmarks)\n",
    "    num_landmarks = len(landmarks)\n",
    "    adjacency_matrix = np.zeros((num_landmarks, num_landmarks), dtype=int)\n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        adjacency_matrix[i, j] = 1\n",
    "        adjacency_matrix[j, i] = 1\n",
    "    return adjacency_matrix\n",
    "\n",
    "adjacency_matrix = construct_adjacency_matrix(landmarks_normalized[0])\n",
    "\n",
    "edge_index = torch.tensor(np.nonzero(adjacency_matrix), dtype=torch.long)\n",
    "\n",
    "batch = torch.zeros(node_features.size(0), dtype=torch.long)\n",
    "\n",
    "data = Data(x=node_features.view(-1, 136), edge_index=edge_index, y=labels, batch=batch)\n",
    "\n",
    "model = GCNWithPooling(num_features=2, num_classes=7)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    loss = loss_fn(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    _, predicted = out.max(dim=1)\n",
    "    print(f\"Predicted Classes: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5aa2aa-0ebd-453e-936b-20d46fe425b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (6868) must match the existing size (101) at non-singleton dimension 0.  Target sizes: [6868, 64].  Tensor sizes: [101, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 138\u001b[0m\n\u001b[0;32m    135\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m    141\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out, data\u001b[38;5;241m.\u001b[39my)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 44\u001b[0m, in \u001b[0;36mGCNWithPooling.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x, edge_index)\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x, batch)\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(x, batch)\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch_geometric\\nn\\pool\\glob.py:63\u001b[0m, in \u001b[0;36mglobal_mean_pool\u001b[1;34m(x, batch, size)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scatter(x, batch, dim\u001b[38;5;241m=\u001b[39mdim, dim_size\u001b[38;5;241m=\u001b[39msize, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch_geometric\\utils\\_scatter.py:82\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[0;32m     79\u001b[0m count\u001b[38;5;241m.\u001b[39mscatter_add_(\u001b[38;5;241m0\u001b[39m, index, src\u001b[38;5;241m.\u001b[39mnew_ones(src\u001b[38;5;241m.\u001b[39msize(dim)))\n\u001b[0;32m     80\u001b[0m count \u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m     83\u001b[0m out \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(size)\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out \u001b[38;5;241m/\u001b[39m broadcast(count, out, dim)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch_geometric\\utils\\_scatter.py:195\u001b[0m, in \u001b[0;36mbroadcast\u001b[1;34m(src, ref, dim)\u001b[0m\n\u001b[0;32m    193\u001b[0m dim \u001b[38;5;241m=\u001b[39m ref\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m+\u001b[39m dim \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dim\n\u001b[0;32m    194\u001b[0m size \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m*\u001b[39m dim) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m+\u001b[39m ((\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m*\u001b[39m (ref\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m-\u001b[39m dim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mview(size)\u001b[38;5;241m.\u001b[39mexpand_as(ref)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (6868) must match the existing size (101) at non-singleton dimension 0.  Target sizes: [6868, 64].  Tensor sizes: [101, 1]"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "class GCNWithPooling(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCNWithPooling, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 256)\n",
    "        self.conv4 = GCNConv(256, 128)\n",
    "        self.conv5 = GCNConv(128, 64)\n",
    "\n",
    "        self.pool1 = global_mean_pool\n",
    "        self.pool2 = global_mean_pool\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(64, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.pool1(x, batch)\n",
    "        x = self.pool2(x, batch)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Helper function to one-hot encode labels\n",
    "emotion_list = [\"Angry\", \"Disgust\", \"Fear\", \"Happiness\", \"Neutral\", \"Sad\", \"Surprised\"]\n",
    "\n",
    "def one_hot_encode(emotion, emotions=emotion_list):\n",
    "    emotion_dict = {emotion: idx for idx, emotion in enumerate(emotions)}\n",
    "    one_hot_vector = [0.0] * len(emotions)\n",
    "    if emotion in emotion_dict:\n",
    "        one_hot_vector[emotion_dict[emotion]] = 1.0\n",
    "    return one_hot_vector\n",
    "\n",
    "# Load and preprocess CSV data\n",
    "csv_path = 'tif_df_train.csv'  # Replace with your actual CSV file path\n",
    "df = pd.read_csv(csv_path)\n",
    "df['Landmarks'] = df['Landmarks'].apply(ast.literal_eval)\n",
    "\n",
    "# One-hot encode labels\n",
    "df['encoded_label'] = df['Labels'].apply(lambda x: one_hot_encode(x))\n",
    "df['encoded_label'] = df['encoded_label'].apply(lambda x: torch.tensor(x, dtype=torch.float32))\n",
    "\n",
    "# Convert the list of encoded labels into a tensor\n",
    "labels = torch.stack([tensor for tensor in df['encoded_label']])\n",
    "\n",
    "# Process landmarks\n",
    "landmarks = df['Landmarks'].apply(lambda x: np.array(x))  # Convert landmarks to numpy arrays\n",
    "landmarks_normalized = np.array([np.array(x) for x in landmarks])  # Convert list of arrays into array\n",
    "\n",
    "# Normalize the landmarks\n",
    "landmarks_reshaped = landmarks_normalized.reshape(-1, 2)  # Flatten the 68x2 landmarks into a 2D array\n",
    "scaler = MinMaxScaler()\n",
    "landmarks_normalized = scaler.fit_transform(landmarks_reshaped)\n",
    "landmarks_normalized = landmarks_normalized.reshape(-1, 68, 2)  # Reshape back to (101, 68, 2)\n",
    "\n",
    "# Prepare node features tensor\n",
    "node_features = torch.tensor(landmarks_normalized, dtype=torch.float32)\n",
    "\n",
    "# Delaunay Triangulation and adjacency matrix generation\n",
    "def delaunay_triangulation(landmarks):\n",
    "    tri = Delaunay(landmarks)\n",
    "    triangles = tri.simplices\n",
    "    edges = set()\n",
    "    for triangle in triangles:\n",
    "        for i in range(3):\n",
    "            edge = tuple(sorted([triangle[i], triangle[(i + 1) % 3]]))\n",
    "            edges.add(edge)\n",
    "    return list(edges)\n",
    "\n",
    "def construct_adjacency_matrix(landmarks):\n",
    "    edges = delaunay_triangulation(landmarks)\n",
    "    num_landmarks = len(landmarks)\n",
    "    adjacency_matrix = np.zeros((num_landmarks, num_landmarks), dtype=int)\n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        adjacency_matrix[i, j] = 1\n",
    "        adjacency_matrix[j, i] = 1\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Generate adjacency matrix (same for all graphs, but could be graph-specific in your case)\n",
    "adjacency_matrix = construct_adjacency_matrix(landmarks_normalized[0])  # Take the first graph to create the adjacency matrix\n",
    "\n",
    "# Create edge index\n",
    "edge_index = torch.tensor(np.nonzero(adjacency_matrix), dtype=torch.long)\n",
    "\n",
    "# Create batch vector (if you have 101 graphs, the batch size will be 101)\n",
    "batch = torch.zeros(node_features.size(0), dtype=torch.long)  # One graph at a time, adjust for multiple graphs if needed\n",
    "\n",
    "# Create Data object for PyTorch Geometric\n",
    "data = Data(x=node_features.view(-1, 2), edge_index=edge_index, y=labels, batch=batch)\n",
    "\n",
    "# Initialize the GCN model\n",
    "model = GCNWithPooling(num_features=2, num_classes=7)  # 2 features (x, y), 7 classes for classification\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(out, data.y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    _, predicted = out.max(dim=1)\n",
    "    print(f\"Predicted Classes: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea85e7f-0ce8-4f00-b45a-567b9bdf84ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
