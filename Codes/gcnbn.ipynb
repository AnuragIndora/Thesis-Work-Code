{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14de407-2006-4858-836c-52cf721fafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch_geometric.data import Data, Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming you have a list of image file paths and corresponding labels\n",
    "# class GraphImageDataset(Dataset):\n",
    "#     def __init__(self, image_paths, labels, transform=None):\n",
    "#         \"\"\"\n",
    "#         Custom dataset for loading graph data from images.\n",
    "#         image_paths: List of paths to the images.\n",
    "#         labels: List of labels for each image (node-level or graph-level).\n",
    "#         transform: Optional transform to apply to the images.\n",
    "#         \"\"\"\n",
    "#         self.image_paths = image_paths\n",
    "#         self.labels = labels\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         # Define edge connections for the graph\n",
    "#         self.connections = [\n",
    "#             # Example of edge connections (based on facial landmarks, modify as needed)\n",
    "#             (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8),\n",
    "#             (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16),\n",
    "#             (17, 18), (18, 19), (19, 20), (20, 21),\n",
    "#             (22, 23), (23, 24), (24, 25), (25, 26),\n",
    "#             (27, 28), (28, 29), (29, 30),\n",
    "#             (30, 31), (31, 32), (32, 33), (33, 34), (34, 35),\n",
    "#             (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 36),\n",
    "#             (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 42),\n",
    "#             (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54),\n",
    "#             (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60),\n",
    "#             (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67), (67, 48),\n",
    "#             (60, 67), (61, 66), (62, 65), (63, 64)\n",
    "#         ]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Load the image\n",
    "#         image_path = self.image_paths[idx]\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "#         # Apply transformations (resize, normalize, etc.)\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        \n",
    "#         # Here, we assume node features are derived from the image (for example, facial landmark coordinates)\n",
    "#         # For simplicity, assume that `image` is used to extract the node features (e.g., landmarks)\n",
    "#         # You would likely use a pre-trained model or a facial landmark detector here\n",
    "#         node_features = self.extract_node_features(image)\n",
    "        \n",
    "#         # Define the edge index (you can adjust the size according to your graph's structure)\n",
    "#         edge_index = torch.tensor(self.connections, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "#         # Get the label (assuming a graph-level classification task here)\n",
    "#         label = self.labels[idx]\n",
    "        \n",
    "#         # Create a Data object to store the graph (nodes and edges)\n",
    "#         data = Data(x=node_features, edge_index=edge_index, y=label)\n",
    "        \n",
    "#         return data\n",
    "    \n",
    "#     def extract_node_features(self, image):\n",
    "#         \"\"\"\n",
    "#         A function to extract node features from the image. Here you would typically use a facial landmark detection method\n",
    "#         to obtain the coordinates of facial landmarks (e.g., eyes, nose, etc.). \n",
    "#         For simplicity, we assume random node features here.\n",
    "#         \"\"\"\n",
    "#         # For now, generating random node features (68 landmarks, 2D coordinates)\n",
    "#         # You should replace this with actual facial landmark detection\n",
    "#         node_features = torch.randn(68, 2)  # Example: 68 landmarks, 2D coordinates\n",
    "        \n",
    "#         return node_features\n",
    "\n",
    "# # Define image transformations (resize, normalization, etc.)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize image\n",
    "#     transforms.ToTensor(),  # Convert to tensor\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "# ])\n",
    "\n",
    "# # # Example usage:\n",
    "# # # List of image paths and corresponding labels\n",
    "# # image_paths = [\"path_to_image1.jpg\", \"path_to_image2.jpg\", \"path_to_image3.jpg\"]\n",
    "# # labels = [0, 1, 2]  # Example labels (could be node-level or graph-level)\n",
    "\n",
    "# # # Create the dataset\n",
    "# # dataset = GraphImageDataset(image_paths=image_paths, labels=labels, transform=transform)\n",
    "\n",
    "# # # Create DataLoader for batching\n",
    "# # data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# # # Example loop through the DataLoader\n",
    "# # for data in data_loader:\n",
    "# #     # data is a batch of graphs\n",
    "# #     # data.x = node features\n",
    "# #     # data.edge_index = edge index\n",
    "# #     # data.y = labels\n",
    "# #     print(f\"Node features: {data.x}\")\n",
    "# #     print(f\"Edge index: {data.edge_index}\")\n",
    "# #     print(f\"Labels: {data.y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5244981f-ec14-4297-8a3d-babc656bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv, TopKPooling, GraphSAGE\n",
    "\n",
    "# class GCNN(nn.Module):\n",
    "#     def __init__(self, num_nodes=68, num_features=2, num_classes=4):\n",
    "#         super(GCNN, self).__init__()\n",
    "        \n",
    "#         # First Graph Convolutional Layer\n",
    "#         self.conv1 = GCNConv(num_features, 64)\n",
    "#         # Second Graph Convolutional Layer\n",
    "#         self.conv2 = GCNConv(64, 64)\n",
    "#         # Third Graph Convolutional Layer\n",
    "#         self.conv3 = GCNConv(64, 64)\n",
    "        \n",
    "#         # Pooling layer to reduce the graph size (optional)\n",
    "#         # Here we use TopKPooling, but you can use other pooling methods like GraphSAGEPooling, etc.\n",
    "#         self.pool1 = TopKPooling(64, ratio=0.5)\n",
    "        \n",
    "#         # Fourth Graph Convolutional Layer\n",
    "#         self.conv4 = GCNConv(64, 128)\n",
    "#         # Fifth Graph Convolutional Layer\n",
    "#         self.conv5 = GCNConv(128, 128)\n",
    "        \n",
    "#         # Second pooling layer\n",
    "#         self.pool2 = TopKPooling(128, ratio=0.5)\n",
    "        \n",
    "#         # Fully Connected layers for classification\n",
    "#         self.fc1 = nn.Linear(128 * num_nodes, 256)\n",
    "#         self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch=None):\n",
    "#         # First set of Graph Convolutional Layers\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.relu(self.conv2(x, edge_index))\n",
    "#         x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "#         # Pooling Layer\n",
    "#         x, edge_index, _, batch, _ = self.pool1(x, edge_index, None, batch)\n",
    "        \n",
    "#         # Second set of Graph Convolutional Layers\n",
    "#         x = F.relu(self.conv4(x, edge_index))\n",
    "#         x = F.relu(self.conv5(x, edge_index))\n",
    "        \n",
    "#         # Pooling Layer\n",
    "#         x, edge_index, _, batch, _ = self.pool2(x, edge_index, None, batch)\n",
    "        \n",
    "#         # Flatten the output for the fully connected layers\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        \n",
    "#         # Fully Connected layers\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)  # Output layer for classification\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# # Example usage of the model\n",
    "# if __name__ == '__main__':\n",
    "#     # Create a sample graph with 68 nodes and 2 features per node\n",
    "#     num_nodes = 68\n",
    "#     num_features = 2\n",
    "#     num_classes = 7\n",
    "    \n",
    "#     # Random node features (num_nodes x num_features)\n",
    "#     x = torch.randn(num_nodes, num_features)\n",
    "    \n",
    "#     # Define the edge connections\n",
    "#     connections = [\n",
    "#         # Jawline\n",
    "#         (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8),\n",
    "#         (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16),\n",
    "#         # Right eyebrow\n",
    "#         (17, 18), (18, 19), (19, 20), (20, 21),\n",
    "#         # Left eyebrow\n",
    "#         (22, 23), (23, 24), (24, 25), (25, 26),\n",
    "#         # Nose bridge\n",
    "#         (27, 28), (28, 29), (29, 30),\n",
    "#         # Nose base\n",
    "#         (30, 31), (31, 32), (32, 33), (33, 34), (34, 35),\n",
    "#         # Right eye\n",
    "#         (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 36),\n",
    "#         # Left eye\n",
    "#         (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 42),\n",
    "#         # Outer lip\n",
    "#         (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54),\n",
    "#         (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60),\n",
    "#         (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67), (67, 48),\n",
    "#         # Inner lip\n",
    "#         (60, 67), (61, 66), (62, 65), (63, 64)\n",
    "#     ]\n",
    "    \n",
    "#     # Convert edge connections into a tensor (edge_index)\n",
    "#     x = torch.randn(68, 2)  # 68 nodes, each with 2 features\n",
    "\n",
    "#     # Labels for classification (assuming 68 nodes with 4 classes for simplicity)\n",
    "#     y = torch.randint(0, 4, (68,))  # Random labels for each node\n",
    "    \n",
    "#     # Create the graph data object (graph representation)\n",
    "#     data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    \n",
    "#     edge_index = torch.tensor(connections, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "#     # Batch information (assuming only 1 graph here)\n",
    "#     batch = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    \n",
    "#     # Create the model\n",
    "#     model = GCNN(num_nodes=num_nodes, num_features=num_features, num_classes=num_classes)\n",
    "    \n",
    "#     # Perform a forward pass\n",
    "#     output = model(x, edge_index, batch)\n",
    "#     print(output.shape)  # Should print torch.Size([68, 4]) as we have 68 nodes and 4 output classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956ff1d-c167-4e93-a980-1d74967f4c54",
   "metadata": {},
   "source": [
    "# Start GraphNeuralNetwork Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29746f86-b73f-448c-b07d-8c8c7c913fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef977ca4-6046-4740-9c20-23aaeea60885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_adjacency_matrix(landmarks):\n",
    "    num_nodes = len(landmarks)\n",
    "    A = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    connections = [\n",
    "        # Jawline\n",
    "        (0, 1),\n",
    "        (1, 2),\n",
    "        (2, 3),\n",
    "        (3, 4),\n",
    "        (4, 5),\n",
    "        (5, 6),\n",
    "        (6, 7),\n",
    "        (7, 8),\n",
    "        (9, 10),\n",
    "        (10, 11),\n",
    "        (11, 12),\n",
    "        (12, 13),\n",
    "        (13, 14),\n",
    "        (14, 15),\n",
    "        (15, 16),\n",
    "        # Right eyebrow\n",
    "        (17, 18),\n",
    "        (18, 19),\n",
    "        (19, 20),\n",
    "        (20, 21),\n",
    "        # Left eyebrow\n",
    "        (22, 23),\n",
    "        (23, 24),\n",
    "        (24, 25),\n",
    "        (25, 26),\n",
    "        # Nose bridge\n",
    "        (27, 28),\n",
    "        (28, 29),\n",
    "        (29, 30),\n",
    "        # Nose base\n",
    "        (30, 31),\n",
    "        (31, 32),\n",
    "        (32, 33),\n",
    "        (33, 34),\n",
    "        (34, 35),\n",
    "        # Right eye\n",
    "        (36, 37),\n",
    "        (37, 38),\n",
    "        (38, 39),\n",
    "        (39, 40),\n",
    "        (40, 41),\n",
    "        (41, 36),\n",
    "        # Left eye\n",
    "        (42, 43),\n",
    "        (43, 44),\n",
    "        (44, 45),\n",
    "        (45, 46),\n",
    "        (46, 47),\n",
    "        (47, 42),\n",
    "        # Outer lip\n",
    "        (48, 49),\n",
    "        (49, 50),\n",
    "        (50, 51),\n",
    "        (51, 52),\n",
    "        (52, 53),\n",
    "        (53, 54),\n",
    "        (54, 55),\n",
    "        (55, 56),\n",
    "        (56, 57),\n",
    "        (57, 58),\n",
    "        (58, 59),\n",
    "        (59, 60),\n",
    "        (60, 61),\n",
    "        (61, 62),\n",
    "        (62, 63),\n",
    "        (63, 64),\n",
    "        (64, 65),\n",
    "        (65, 66),\n",
    "        (66, 67),\n",
    "        (67, 48),\n",
    "        # Inner lip\n",
    "        (60, 67),\n",
    "        (61, 66),\n",
    "        (62, 65),\n",
    "        (63, 64),\n",
    "    ]\n",
    "\n",
    "    for i, j in connections:\n",
    "        if i < num_nodes and j < num_nodes:\n",
    "            A[i, j] = 1\n",
    "            A[j, i] = 1\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee2e0d7-a494-4799-a012-71b6b1cece42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency Matrix:\n",
      "[[0 1 1 0 1]\n",
      " [1 0 0 1 1]\n",
      " [1 0 0 1 1]\n",
      " [0 1 1 0 1]\n",
      " [1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "def delaunay_triangulation(landmarks):\n",
    "    \"\"\"\n",
    "    Perform Delaunay triangulation and return triangles and edges.\n",
    "    \"\"\"\n",
    "    tri = Delaunay(landmarks)\n",
    "    triangles = tri.simplices\n",
    "    edges = set()\n",
    "    for triangle in triangles:\n",
    "        for i in range(3):\n",
    "            edge = tuple(sorted([triangle[i], triangle[(i + 1) % 3]]))\n",
    "            edges.add(edge)\n",
    "    return triangles, list(edges)\n",
    "\n",
    "def construct_adjacency_matrix(landmarks):\n",
    "    \"\"\"\n",
    "    Construct an adjacency matrix from the Delaunay triangulation of the landmarks.\n",
    "    \n",
    "    Args:\n",
    "        landmarks (ndarray): Array of 2D points (n, 2) representing the landmarks.\n",
    "    \n",
    "    Returns:\n",
    "        adjacency_matrix (ndarray): A binary adjacency matrix of shape (n, n).\n",
    "    \"\"\"\n",
    "    _, edges = delaunay_triangulation(landmarks)\n",
    "    num_landmarks = len(landmarks)\n",
    "    adjacency_matrix = np.zeros((num_landmarks, num_landmarks), dtype=int)\n",
    "    \n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        adjacency_matrix[i, j] = 1\n",
    "        adjacency_matrix[j, i] = 1  # Symmetric for undirected graph\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Example usage\n",
    "landmarks = np.array([[0, 0], [1, 0], [0, 1], [1, 1], [0.5, 0.5]])  # Sample landmarks\n",
    "adjacency_matrix = construct_adjacency_matrix(landmarks)\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e723a14-e634-42a2-b2f7-0d7d209a1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_node_adjacency_matrix(landmarks):\n",
    "    num_landmarks = len(landmarks)\n",
    "    master_node = 31\n",
    "    adjacency_matrix = np.zeros((num_landmarks, num_landmarks), dtype=int)\n",
    "\n",
    "    for i in range(num_landmarks):\n",
    "        if i != master_node:\n",
    "            adjacency_matrix[master_node, i] = 1\n",
    "            adjacency_matrix[i, master_node] = 1\n",
    "\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83e0424-e17b-4313-bb84-49a1514995b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency_matrix(A):\n",
    "    A = A + np.eye(A.shape[0])  # Add self-connections\n",
    "    D = np.diag(np.sum(A, axis=1))\n",
    "    D_inv_sqrt = np.linalg.inv(np.sqrt(D))\n",
    "    A_normalized = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return A_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9361623a-c83a-449e-84fd-b88e2fdfafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, GraphSAGE\n",
    "\n",
    "# class GCNN(nn.Module):\n",
    "#     def __init__(self, num_nodes=68, num_features=2, num_classes=4):\n",
    "#         super(GCNN, self).__init__()\n",
    "        \n",
    "#         # First Graph Convolutional Layer\n",
    "#         self.conv1 = GCNConv(num_features, 64)\n",
    "#         # Second Graph Convolutional Layer\n",
    "#         self.conv2 = GCNConv(64, 64)\n",
    "#         # Third Graph Convolutional Layer\n",
    "#         self.conv3 = GCNConv(64, 64)\n",
    "        \n",
    "#         # Pooling layer to reduce the graph size (optional)\n",
    "#         # Here we use TopKPooling, but you can use other pooling methods like GraphSAGEPooling, etc.\n",
    "#         self.pool1 = TopKPooling(64, ratio=0.5)\n",
    "        \n",
    "#         # Fourth Graph Convolutional Layer\n",
    "#         self.conv4 = GCNConv(64, 128)\n",
    "#         # Fifth Graph Convolutional Layer\n",
    "#         self.conv5 = GCNConv(128, 128)\n",
    "        \n",
    "#         # Second pooling layer\n",
    "#         self.pool2 = TopKPooling(128, ratio=0.5)\n",
    "        \n",
    "#         # Fully Connected layers for classification\n",
    "#         self.fc1 = nn.Linear(128 * num_nodes, 256)\n",
    "#         self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch=None):\n",
    "#         # First set of Graph Convolutional Layers\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.relu(self.conv2(x, edge_index))\n",
    "#         x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "#         # Pooling Layer\n",
    "#         x, edge_index, _, batch, _ = self.pool1(x, edge_index, None, batch)\n",
    "        \n",
    "#         # Second set of Graph Convolutional Layers\n",
    "#         x = F.relu(self.conv4(x, edge_index))\n",
    "#         x = F.relu(self.conv5(x, edge_index))\n",
    "        \n",
    "#         # Pooling Layer\n",
    "#         x, edge_index, _, batch, _ = self.pool2(x, edge_index, None, batch)\n",
    "        \n",
    "#         # Flatten the output for the fully connected layers\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        \n",
    "#         # Fully Connected layers\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)  # Output layer for classification\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d8f17a-ba86-4a2b-9b85-bc4e1ab2c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv, TopKPooling\n",
    "\n",
    "# class GCNN(nn.Module):\n",
    "#     def __init__(self, num_nodes=68, num_features=2, num_classes=7):\n",
    "#         super(GCNN, self).__init__()\n",
    "        \n",
    "#         # First Graph Convolutional Layer\n",
    "#         self.conv1 = GCNConv(num_features, 64)  # Input: (num_nodes, num_features) -> Output: (num_nodes, 64)\n",
    "#         # Second Graph Convolutional Layer\n",
    "#         self.conv2 = GCNConv(64, 64)             # Input: (num_nodes, 64) -> Output: (num_nodes, 64)\n",
    "#         # Third Graph Convolutional Layer\n",
    "#         self.conv3 = GCNConv(64, 64)             # Input: (num_nodes, 64) -> Output: (num_nodes, 64)\n",
    "        \n",
    "#         # Pooling layer to reduce the graph size\n",
    "#         self.pool1 = TopKPooling(64, ratio=0.5)  # Input: (num_nodes, 64) -> Output: (num_pooled_nodes, 64)\n",
    "        \n",
    "#         # Fourth Graph Convolutional Layer\n",
    "#         self.conv4 = GCNConv(64, 128)            # Input: (num_pooled_nodes, 64) -> Output: (num_pooled_nodes, 128)\n",
    "#         # Fifth Graph Convolutional Layer\n",
    "#         self.conv5 = GCNConv(128, 128)           # Input: (num_pooled_nodes, 128) -> Output: (num_pooled_nodes, 128)\n",
    "        \n",
    "#         # Second pooling layer\n",
    "#         self.pool2 = TopKPooling(128, ratio=0.5) # Input: (num_pooled_nodes, 128) -> Output: (num_pooled_nodes2, 128)\n",
    "        \n",
    "#         # Fully Connected layers for classification\n",
    "#         self.fc1 = None  # Initialize as None\n",
    "#         self.fc2 = nn.Linear(256, num_classes)  # Output layer: Input: (batch_size, 256) -> Output: (batch_size, num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch=None):\n",
    "#         # before applying convolutional layer the size of the x and edge index \n",
    "#         print(f\"After conv1: x size = {x.size()}, edge_index size = {edge_index.size()}\")\n",
    "        \n",
    "#         # First set of Graph Convolutional Layers\n",
    "#         x = F.relu(self.conv1(x, edge_index))  # After conv1: x size = (num_nodes, 64)\n",
    "#         print(f\"After conv1: x size = {x.size()}, edge_index size = {edge_index.size()}\")\n",
    "        \n",
    "#         x = F.relu(self.conv2(x, edge_index))  # After conv2: x size = (num_nodes, 64)\n",
    "#         print(f\"After conv2: x size = {x.size()}, edge_index size = {edge_index.size()}\")\n",
    "        \n",
    "#         x = F.relu(self.conv3(x, edge_index))  # After conv3: x size = (num_nodes, 64)\n",
    "#         print(f\"After conv3: x size = {x.size()}, edge_index size = {edge_index.size()}\")\n",
    "        \n",
    "#         # Pooling Layer\n",
    "#         x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)  # After pool1: x size = (num_pooled_nodes, 64)\n",
    "#         print(f\"After pool1: x size = {x.size()}, edge_index size = {edge_index.size()}, batch size = {batch.size()}\")\n",
    "        \n",
    "#         # Second set of Graph Convolutional Layers\n",
    "#         x = F.relu(self.conv4(x, edge_index))  # After conv4: x size = (num_pooled_nodes, 128)\n",
    "#         print(f\"After conv4: x size = {x.size()}, edge_index size = {edge_index.size()}\")\n",
    "        \n",
    "#         x = F.relu(self.conv5(x, edge_index))  # After conv5: x size = (num_pooled_nodes, 128)\n",
    "#         print(f\"After conv5: x size = {x.size()}, edge_index size = {edge_index.size()}\")\n",
    "        \n",
    "#         # Pooling Layer\n",
    "#         x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)  # After pool2: x size = (num_pooled_nodes2, 128)\n",
    "#         print(f\"After pool2: x size = {x.size()}, edge_index size = {edge_index.size()}, batch size = {batch.size()}\")\n",
    "        \n",
    "#         # Flatten the output for the fully connected layers\n",
    "#         x = x.view(x.size(0), -1)  # After flattening: x size = (batch_size, 128 * num_pooled_nodes2)\n",
    "#         print(f\"After flattening: x size = {x.size()}\")\n",
    "        \n",
    "#         # Define fc1 based on the flattened size\n",
    "#         if self.fc1 is None:\n",
    "#             self.fc1 = nn.Linear(x.size(1), 256)  # Set input size dynamically: Input: (batch_size, 128 * num_pooled_nodes2) -> Output: (batch_size, 256)\n",
    "\n",
    "#         # Fully Connected layers\n",
    "#         x = F.relu(self.fc1(x))  # After fc1: x size = (batch_size, 256)\n",
    "#         print(f\"After fc1: x size = {x.size()}\")\n",
    "        \n",
    "#         x = self.fc2(x)  # Output layer for classification: Input: (batch_size, 256) -> Output: (batch_size, num_classes)\n",
    "#         print(f\"After fc2: x size = {x.size()}\")\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d69bfb5c-6eff-4113-9911-64a805213363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling\n",
    "\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, num_nodes=68, num_features=2, num_classes=7):\n",
    "        super(GCNN, self).__init__()\n",
    "        \n",
    "        # First Graph Convolutional Layer\n",
    "        self.conv1 = GCNConv(num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        self.conv3 = GCNConv(64, 64)\n",
    "        \n",
    "        # Pooling layer to reduce the graph size\n",
    "        self.pool1 = TopKPooling(64, ratio=0.5)\n",
    "        \n",
    "        # Fourth Graph Convolutional Layer\n",
    "        self.conv4 = GCNConv(64, 128)\n",
    "        self.conv5 = GCNConv(128, 128)\n",
    "        \n",
    "        # Second pooling layer\n",
    "        self.pool2 = TopKPooling(128, ratio=0.5)\n",
    "        \n",
    "        # Fully Connected layers for classification\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Output layer: Input: (batch_size, 256) -> Output: (batch_size, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # First set of Graph Convolutional Layers\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        \n",
    "        # Second set of Graph Convolutional Layers\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        \n",
    "        # Pooling Layer\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flattening\n",
    "        \n",
    "        # Define fc1 based on the flattened size\n",
    "        fc1_input_size = x.size(1)  # Get the actual input size dynamically\n",
    "        self.fc1 = nn.Linear(fc1_input_size, 256)  # Set input size dynamically\n",
    "\n",
    "        # Fully Connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dbb81f6-5aec-431b-baed-e4b45309b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82a47ed-b2a8-41ee-82b0-b68689d96ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from CSV\n",
    "def load_data_from_csv(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    images = []\n",
    "    landmarks = []\n",
    "    expressions = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        image_name = row['image_name']\n",
    "        \n",
    "        # Convert string representation of list into actual list\n",
    "        landmark_points = ast.literal_eval(row['landmark_points'])  # Convert string to list\n",
    "        expression = np.array(ast.literal_eval(row['expressions']))  # Ensure it's a numpy array from string\n",
    "\n",
    "        # Convert landmark points to numpy array\n",
    "        landmarks.append(np.array(landmark_points))\n",
    "        expressions.append(expression)\n",
    "        images.append(image_name)\n",
    "        \n",
    "    return images, landmarks, expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e35d756-db82-441a-917e-cdbf6c9f245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert landmarks and labels into PyTorch tensors\n",
    "def create_graph_data(landmarks, expression, num_nodes=68):\n",
    "    \"\"\"\n",
    "    Create a Data object for PyTorch Geometric.\n",
    "    \n",
    "    Args:\n",
    "        landmarks (numpy.ndarray): Landmark points (shape: [num_landmarks, 2]).\n",
    "        expression (numpy.ndarray): One-hot encoded expression (shape: [num_classes]).\n",
    "        \n",
    "    Returns:\n",
    "        data (torch_geometric.data.Data): PyTorch Geometric Data object.\n",
    "    \"\"\"\n",
    "    # Feature matrix (landmark points)\n",
    "    x = torch.tensor(landmarks, dtype=torch.float)\n",
    "    \n",
    "    # Create adjacency matrix using the function defined earlier\n",
    "    adjacency_matrix = construct_adjacency_matrix(landmarks)\n",
    "    \n",
    "    # Edge index (convert adjacency matrix to edge list)\n",
    "    edge_index = torch.tensor(np.nonzero(adjacency_matrix), dtype=torch.long)\n",
    "    \n",
    "    # Label (expression one-hot encoded)\n",
    "    y = torch.tensor(expression, dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9071b73-16af-49bd-9143-a186f917be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV data\n",
    "train_images, train_landmarks, train_expressions = load_data_from_csv('tif_df_train_image.csv')  # Update with actual path\n",
    "test_images, test_landmarks, test_expressions = load_data_from_csv('tif_df_test_image.csv')  # Update with actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb63ab88-d749-4b23-9ec8-e4aa7f12f4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73485/2487346700.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  edge_index = torch.tensor(np.nonzero(adjacency_matrix), dtype=torch.long)\n",
      "/raid/home/dgx1575/20je0167_anurag/.venv/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Create graph data for the dataset\n",
    "train_dataset = [create_graph_data(l, exp) for l, exp in zip(train_landmarks, train_expressions)]\n",
    "test_dataset = [create_graph_data(l, exp) for l, exp in zip(test_landmarks, test_expressions)]\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model Initialization\n",
    "model = GCNN(num_nodes=68, num_features=2, num_classes=7).to('cuda:1')  # Assuming GPU training\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Binary Cross Entropy with logits for multi-class one-hot targets\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39c78fa-9d5c-4e27-a3da-1140bc790cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with Validation\n",
    "def train_and_validate(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data.x, data.edge_index, batch=data.batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted = (output > 0.5).float()  # Apply threshold for binary classification\n",
    "        correct += (predicted == data.y).sum().item()\n",
    "        total += data.y.size(0) * data.y.size(1)  # For multi-class (one-hot), multiply by number of classes\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch} - Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1060fe46-9899-4bbe-9a08-8e81ac508abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|                                                                                                              | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, data\u001b[38;5;241m.\u001b[39my)\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m, in \u001b[0;36mGCNN.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(fc1_input_size, \u001b[38;5;241m256\u001b[39m)  \u001b[38;5;66;03m# Set input size dynamically\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Fully Connected layers\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/20je0167_anurag/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Send the model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_and_validate(model, train_loader, criterion, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4810e-1d4d-47f2-a95a-5102eca1308b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9c385-9e1d-4865-bc90-f4f13e8003b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
