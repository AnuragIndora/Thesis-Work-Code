{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import random\n",
    "from PIL import Image\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81214d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906efe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "# dataset = 'TIF_DF'\n",
    "dataset = 'ChildNet'\n",
    "print(os.listdir(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = os.listdir(dataset + \"/train\")\n",
    "classes = os.listdir(dataset + \"/Train\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d279636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(dataset + \"/Test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = dataset\n",
    "train_dir = data_dir + '/Train'\n",
    "test_dir = data_dir + '/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf38483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class distribution\n",
    "count = []\n",
    "for folder in classes:\n",
    "    num_images_train = len(os.listdir(train_dir + '/' + folder))\n",
    "    num_images_test = len(os.listdir(test_dir + '/' + folder))\n",
    "    count.append(num_images_train)\n",
    "    print(f'Training Set: {folder} = {num_images_train}')\n",
    "    print(f'Testing Set: {folder} = {num_images_test}')\n",
    "    print(\"--\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device (GPU or CPU)\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "print(get_default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72627ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            # Check if we have a tuple of 4 elements (img1, img2, labels, metadata)\n",
    "            if isinstance(b, tuple) and len(b) == 4:\n",
    "                img1, img2, labels, metadata = b\n",
    "                # Only move tensors to device, leave metadata as is\n",
    "                yield to_device(img1, self.device), to_device(img2, self.device), to_device(labels, self.device), metadata\n",
    "            else:\n",
    "                # For other types of batches\n",
    "                yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bec3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations with data augmentation\n",
    "stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet stats for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d96462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomCrop(64, padding=4, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971891ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ce83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Siamese Dataset that works with emotion classes\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        # Create a list of all image paths and their labels\n",
    "        self.images = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(root_dir, cls)\n",
    "            class_idx = self.class_to_idx[cls]\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                self.images.append((img_path, class_idx, cls))  # Store class name as well\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img1_class, img1_class_name = self.images[idx]\n",
    "\n",
    "        # Decide whether to create a similar (same class) or dissimilar (different class) pair\n",
    "        should_get_same_class = random.random() < 0.5\n",
    "\n",
    "        if should_get_same_class:\n",
    "            # Find all images of the same class\n",
    "            same_class_images = [(p, c, n) for p, c, n in self.images if c == img1_class and p != img1_path]\n",
    "            if same_class_images:\n",
    "                img2_path, img2_class, img2_class_name = random.choice(same_class_images)\n",
    "            else:\n",
    "                # If no other images in the same class, use the same image\n",
    "                img2_path, img2_class, img2_class_name = img1_path, img1_class, img1_class_name\n",
    "\n",
    "            # Label 1 indicates same class\n",
    "            pair_label = torch.tensor(1.0, dtype=torch.float)\n",
    "        else:\n",
    "            # Find all images of different classes\n",
    "            diff_class_images = [(p, c, n) for p, c, n in self.images if c != img1_class]\n",
    "            img2_path, img2_class, img2_class_name = random.choice(diff_class_images)\n",
    "\n",
    "            # Label 0 indicates different class\n",
    "            pair_label = torch.tensor(0.0, dtype=torch.float)\n",
    "\n",
    "        # Load images\n",
    "        img1 = Image.open(img1_path).convert('RGB')\n",
    "        img2 = Image.open(img2_path).convert('RGB')\n",
    "\n",
    "        # Apply transforms if available\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        # Return metadata for visualization\n",
    "        metadata = {\n",
    "            'img1_class': img1_class_name,\n",
    "            'img2_class': img2_class_name,\n",
    "            'img1_path': img1_path,\n",
    "            'img2_path': img2_path\n",
    "        }\n",
    "\n",
    "        return img1, img2, pair_label, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified DataLoader to handle metadata\n",
    "class SiameseDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, num_workers=0, **kwargs):\n",
    "        super(SiameseDataLoader, self).__init__(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "            num_workers=num_workers, collate_fn=self.collate_fn, **kwargs\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imgs1 = []\n",
    "        imgs2 = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "\n",
    "        for img1, img2, label, meta in batch:\n",
    "            imgs1.append(img1)\n",
    "            imgs2.append(img2)\n",
    "            labels.append(label)\n",
    "            metadata.append(meta)\n",
    "\n",
    "        return torch.stack(imgs1), torch.stack(imgs2), torch.stack(labels), metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Network Base Class\n",
    "class SiameseBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        img1, img2, labels, _ = batch  # Ignore metadata for training\n",
    "        output = self(img1, img2)\n",
    "        loss = F.binary_cross_entropy(output, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        img1, img2, labels, _ = batch  # Ignore metadata for validation\n",
    "        output = self(img1, img2)\n",
    "        loss = F.binary_cross_entropy(output, labels)\n",
    "\n",
    "        # Calculate accuracy (threshold at 0.5)\n",
    "        predictions = (output > 0.5).float()\n",
    "        correct = torch.eq(predictions, labels).sum().item()\n",
    "        acc = correct / len(labels)\n",
    "\n",
    "        return {'val_loss': loss.detach(), 'val_acc': torch.tensor(acc, device=loss.device)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result, train_loss=None):\n",
    "        if train_loss:\n",
    "            print(\n",
    "                f\"Epoch {epoch}: train_loss: {train_loss:.4f}, val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese ResNet152 Model\n",
    "class SiameseResNet50(SiameseBase):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the feature extractor using ResNet152 without the final FC layer\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        modules = list(resnet.children())[:-1]  # Remove the final FC layer\n",
    "        self.feature_extractor = nn.Sequential(*modules)\n",
    "\n",
    "        # # Freeze early layers for transfer learning\n",
    "        # for param in list(self.feature_extractor.parameters())[:-30]:\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # Freeze all layers first\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.feature_extractor[-3].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Unfreeze last residual block (layer3)\n",
    "        for param in self.feature_extractor[-2].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Unfreeze last residual block (layer4)\n",
    "        for param in self.feature_extractor[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "        # Layer to compute similarity\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        \"\"\"Forward pass for one input\"\"\"\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"Forward pass for the siamese network\"\"\"\n",
    "        # Extract features from both inputs\n",
    "        feat1 = self.forward_one(x1)\n",
    "        feat2 = self.forward_one(x2)\n",
    "\n",
    "        # Compute absolute difference between features\n",
    "        diff = torch.abs(feat1 - feat2)\n",
    "\n",
    "        # Compute similarity score\n",
    "        out = self.fc(diff)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function for validation\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e521a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get learning rate from optimizer\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e96d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training and validation metrics\n",
    "def plot_metrics(history):\n",
    "    train_losses = [x.get('train_loss', 0) for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    val_accs = [x['val_acc'] for x in history]\n",
    "\n",
    "    # Create figure with 2 subplots\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses[1:], label='Train Loss')  # Skip the first one as it may not have train loss\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('siamese_training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with one-cycle learning rate schedule\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()  # Clear GPU memory\n",
    "    history = []\n",
    "\n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "\n",
    "    # For tracking learning rates\n",
    "    lrs = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        # Create progress bar\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "\n",
    "        for batch in loop:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping if specified\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "\n",
    "        # Record training loss\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "\n",
    "        # Record learning rates for this epoch\n",
    "        result['lrs'] = lrs\n",
    "\n",
    "        # Print epoch results\n",
    "        model.epoch_end(epoch, result, result['train_loss'])\n",
    "\n",
    "        # Save history\n",
    "        history.append(result)\n",
    "\n",
    "        # Save model after each epoch\n",
    "        torch.save(model.state_dict(), 'latest_siamese_model.pth')\n",
    "\n",
    "        # Save best model if validation accuracy improves\n",
    "        if epoch == 0 or result['val_acc'] > max([h['val_acc'] for h in history[:-1]]):\n",
    "            torch.save(model.state_dict(), 'best_siamese_model.pth')\n",
    "            print(f\"Model saved at epoch {epoch} with val_acc: {result['val_acc']:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14069967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the test set with emotion classes\n",
    "def evaluate_siamese_test_set(model, test_dl, classes):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_metadata = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl, desc=\"Evaluating test set\"):\n",
    "            img1, img2, labels, metadata = batch\n",
    "            outputs = model(img1, img2)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_metadata.extend(metadata)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Convert predictions to binary (0 or 1) using 0.5 threshold\n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(all_labels, binary_preds)\n",
    "    print(f'Test Accuracy: {acc * 100:.2f}%')\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(all_labels, binary_preds, target_names=['Different', 'Same']))\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, binary_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=['Different', 'Same'],\n",
    "                yticklabels=['Different', 'Same'])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('siamese_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate ROC curve and AUC\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('siamese_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze class-wise performance\n",
    "    print(\"\\nEmotion class similarity analysis:\")\n",
    "\n",
    "    # Count correct predictions for each class pair\n",
    "    class_pair_counts = {}\n",
    "    class_pair_correct = {}\n",
    "\n",
    "    for i in range(len(all_labels)):\n",
    "        img1_class = all_metadata[i]['img1_class']\n",
    "        img2_class = all_metadata[i]['img2_class']\n",
    "        pair_key = f\"{img1_class}-{img2_class}\"\n",
    "\n",
    "        # Initialize if not seen before\n",
    "        if pair_key not in class_pair_counts:\n",
    "            class_pair_counts[pair_key] = 0\n",
    "            class_pair_correct[pair_key] = 0\n",
    "\n",
    "        class_pair_counts[pair_key] += 1\n",
    "        if all_labels[i] == binary_preds[i]:\n",
    "            class_pair_correct[pair_key] += 1\n",
    "\n",
    "    # Calculate and print accuracy for each class pair\n",
    "    print(\"\\nPair-wise accuracies:\")\n",
    "    class_pair_acc = {}\n",
    "    for pair, count in class_pair_counts.items():\n",
    "        if count > 0:\n",
    "            acc = class_pair_correct[pair] / count\n",
    "            class_pair_acc[pair] = acc\n",
    "            print(f\"{pair}: {acc:.4f} ({class_pair_correct[pair]}/{count})\")\n",
    "\n",
    "    # Create a matrix of similarities between classes\n",
    "    similarity_matrix = np.zeros((len(classes), len(classes)))\n",
    "    class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "    for i in range(len(all_metadata)):\n",
    "        if all_labels[i] == 1:  # Only for pairs that should be similar\n",
    "            cls1 = all_metadata[i]['img1_class']\n",
    "            cls2 = all_metadata[i]['img2_class']\n",
    "            similarity = all_preds[i]\n",
    "\n",
    "            idx1 = class_to_idx[cls1]\n",
    "            idx2 = class_to_idx[cls2]\n",
    "\n",
    "            # Update similarity (we'll average it later)\n",
    "            similarity_matrix[idx1, idx2] += similarity\n",
    "            similarity_matrix[idx2, idx1] += similarity  # Make it symmetric\n",
    "\n",
    "    # Average similarities\n",
    "    class_pair_counts_matrix = np.zeros((len(classes), len(classes)))\n",
    "    for i in range(len(all_metadata)):\n",
    "        if all_labels[i] == 1:\n",
    "            cls1 = all_metadata[i]['img1_class']\n",
    "            cls2 = all_metadata[i]['img2_class']\n",
    "            idx1 = class_to_idx[cls1]\n",
    "            idx2 = class_to_idx[cls2]\n",
    "            class_pair_counts_matrix[idx1, idx2] += 1\n",
    "            class_pair_counts_matrix[idx2, idx1] += 1\n",
    "\n",
    "    # Avoid division by zero\n",
    "    class_pair_counts_matrix = np.maximum(class_pair_counts_matrix, 1)\n",
    "    similarity_matrix = similarity_matrix / class_pair_counts_matrix\n",
    "\n",
    "    # Plot similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel(\"Emotion Class\")\n",
    "    plt.ylabel(\"Emotion Class\")\n",
    "    plt.title(\"Emotion Class Similarity Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('emotion_similarity_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return acc, cm, roc_auc, similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize sample pair predictions with emotion classes\n",
    "def visualize_pair_predictions(model, test_dl, classes, num_samples=5):\n",
    "    # Get a batch of image pairs\n",
    "    dataiter = iter(test_dl)\n",
    "    img1, img2, labels, metadata = next(dataiter)\n",
    "\n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img1, img2)\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "    # Convert tensors to CPU\n",
    "    img1 = img1.cpu()\n",
    "    img2 = img2.cpu()\n",
    "    labels = labels.cpu()\n",
    "    outputs = outputs.cpu()\n",
    "    preds = preds.cpu()\n",
    "\n",
    "    # Function to denormalize images for display\n",
    "    def denormalize(image, mean=stats[0], std=stats[1]):\n",
    "        img_denorm = image.clone()\n",
    "        for i in range(3):\n",
    "            img_denorm[i] = img_denorm[i] * std[i] + mean[i]\n",
    "        return torch.clamp(img_denorm, 0, 1)\n",
    "\n",
    "    # Plot the pairs with predictions\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(min(num_samples, len(img1))):\n",
    "        # Denormalize images\n",
    "        img1_display = denormalize(img1[i])\n",
    "        img2_display = denormalize(img2[i])\n",
    "\n",
    "        # Convert to numpy for matplotlib\n",
    "        img1_display = img1_display.permute(1, 2, 0).numpy()\n",
    "        img2_display = img2_display.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Plot image pair\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        plt.imshow(img1_display)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Image 1: {metadata[i]['img1_class']}\")\n",
    "\n",
    "        plt.subplot(2, num_samples, i + 1 + num_samples)\n",
    "        plt.imshow(img2_display)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Image 2: {metadata[i]['img2_class']}\")\n",
    "\n",
    "        # Add prediction information\n",
    "        true_label = \"Same\" if labels[i] == 1 else \"Different\"\n",
    "        pred_label = \"Same\" if preds[i] == 1 else \"Different\"\n",
    "        color = 'green' if labels[i] == preds[i] else 'red'\n",
    "        similarity_score = outputs[i].item()\n",
    "\n",
    "        plt.figtext(0.1 + (i * 0.2), 0.01,\n",
    "                    f\"True: {true_label}\\nPred: {pred_label}\\nScore: {similarity_score:.2f}\",\n",
    "                    color=color, ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.savefig('siamese_sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cbad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Required for Windows to avoid the multiprocessing error\n",
    "\n",
    "    # Create Siamese datasets with metadata\n",
    "    train_ds = SiameseDataset(train_dir, train_transforms)\n",
    "    test_ds = SiameseDataset(test_dir, test_transforms)\n",
    "\n",
    "    # Create data loaders with custom collate function\n",
    "    batch_size = 64\n",
    "    train_dl = SiameseDataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_dl = SiameseDataLoader(test_ds, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Move data to device\n",
    "    train_dl = DeviceDataLoader(train_dl, device)\n",
    "    test_dl = DeviceDataLoader(test_dl, device)\n",
    "\n",
    "    # Create Siamese model\n",
    "    model = SiameseResNet50(pretrained=True)\n",
    "    model = to_device(model, device)\n",
    "\n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")\n",
    "\n",
    "    # Get initial validation metrics\n",
    "    initial_result = evaluate(model, test_dl)\n",
    "    print(\"Initial validation metrics:\", initial_result)\n",
    "\n",
    "    # Train the model\n",
    "    history = fit_one_cycle(\n",
    "        epochs=30,  # Reduced epochs for siamese network\n",
    "        max_lr=0.001,\n",
    "        model=model,\n",
    "        train_loader=train_dl,\n",
    "        val_loader=test_dl,\n",
    "        weight_decay=0.01,\n",
    "        grad_clip=0.1,\n",
    "        opt_func=torch.optim.Adam\n",
    "    )\n",
    "\n",
    "    # Plot training metrics\n",
    "    plot_metrics(history)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'final_siamese_model.pth')\n",
    "\n",
    "    # Load the best model for evaluation\n",
    "    model.load_state_dict(torch.load('best_siamese_model.pth'))\n",
    "    model = to_device(model, device)\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    test_acc, conf_matrix, roc_auc, similarity_matrix = evaluate_siamese_test_set(model, test_dl, classes)\n",
    "\n",
    "    # Visualize some predictions with emotion classes\n",
    "    visualize_pair_predictions(model, test_dl, classes, num_samples=5)\n",
    "\n",
    "    # Plot learning rate vs. loss\n",
    "    if len(history) > 0 and 'lrs' in history[0]:\n",
    "        # Extract learning rates and losses\n",
    "        epochs_lrs = []\n",
    "        epochs_losses = []\n",
    "\n",
    "        for epoch_result in history:\n",
    "            if 'lrs' in epoch_result and 'train_loss' in epoch_result:\n",
    "                # We'll use the average LR for the epoch\n",
    "                avg_lr = sum(epoch_result['lrs']) / len(epoch_result['lrs'])\n",
    "                epochs_lrs.append(avg_lr)\n",
    "                epochs_losses.append(epoch_result['train_loss'])\n",
    "\n",
    "        # Plot LR vs. Loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs_lrs, epochs_losses, 'o-')\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate (log scale)')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title('Learning Rate vs. Training Loss')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.savefig('siamese_lr_vs_loss.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # This is the key to fixing the multiprocessing error\n",
    "    multiprocessing.freeze_support()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deac150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import random\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Define the emotional dimensions: Valence and Arousal\n",
    "def get_emotion_block(emotion):\n",
    "    \"\"\"\n",
    "    Classifies emotions into 4 blocks based on Valence and Arousal.\n",
    "    \"\"\"\n",
    "    # Define the mapping based on valence and arousal\n",
    "    positive_high_arousal = [\"Happiness\", \"Surprised\"]\n",
    "    positive_low_arousal = [\"Neutral\"]\n",
    "    negative_high_arousal = [\"Angry\", \"Fear\"]\n",
    "    negative_low_arousal = [\"Sad\", \"Disgust\"]\n",
    "    \n",
    "    if emotion in positive_high_arousal:\n",
    "        return \"Positive High Arousal\"\n",
    "    elif emotion in positive_low_arousal:\n",
    "        return \"Positive Low Arousal\"\n",
    "    elif emotion in negative_high_arousal:\n",
    "        return \"Negative High Arousal\"\n",
    "    elif emotion in negative_low_arousal:\n",
    "        return \"Negative Low Arousal\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Dataset paths\n",
    "dataset = 'TIF_DF'\n",
    "print(os.listdir(dataset))\n",
    "\n",
    "raw_classes = os.listdir(dataset + \"/train\")\n",
    "print(\"Original emotion classes:\", raw_classes)\n",
    "\n",
    "# Convert raw emotion classes to emotion blocks\n",
    "emotion_block_mapping = {emotion: get_emotion_block(emotion) for emotion in raw_classes}\n",
    "print(\"Emotion block mapping:\", emotion_block_mapping)\n",
    "\n",
    "# Get unique emotion blocks\n",
    "emotion_blocks = list(set(emotion_block_mapping.values()))\n",
    "print(\"Emotion blocks:\", emotion_blocks)\n",
    "\n",
    "data_dir = dataset\n",
    "train_dir = data_dir + '/train'\n",
    "test_dir = data_dir + '/test'\n",
    "\n",
    "# Print class distribution with emotion blocks\n",
    "count = []\n",
    "for folder in raw_classes:\n",
    "    num_images_train = len(os.listdir(train_dir + '/' + folder))\n",
    "    num_images_test = len(os.listdir(test_dir + '/' + folder))\n",
    "    count.append(num_images_train)\n",
    "    emotion_block = get_emotion_block(folder)\n",
    "    print(f'Training Set: {folder} (Block: {emotion_block}) = {num_images_train}')\n",
    "    print(f'Testing Set: {folder} (Block: {emotion_block}) = {num_images_test}')\n",
    "    print(\"--\" * 10)\n",
    "\n",
    "\n",
    "# Define device (GPU or CPU)\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            # Check if we have a tuple of 4 elements (img1, img2, labels, metadata)\n",
    "            if isinstance(b, tuple) and len(b) == 4:\n",
    "                img1, img2, labels, metadata = b\n",
    "                # Only move tensors to device, leave metadata as is\n",
    "                yield to_device(img1, self.device), to_device(img2, self.device), to_device(labels, self.device), metadata\n",
    "            else:\n",
    "                # For other types of batches\n",
    "                yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Image transformations with data augmentation\n",
    "stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet stats for normalization\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomCrop(64, padding=4, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])\n",
    "\n",
    "\n",
    "# Modified Siamese Dataset that works with emotion blocks\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, emotion_block_mapping=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.emotion_block_mapping = emotion_block_mapping\n",
    "        self.raw_classes = os.listdir(root_dir)\n",
    "        \n",
    "        # Get emotion blocks from raw classes\n",
    "        self.emotion_blocks = list(set([get_emotion_block(cls) for cls in self.raw_classes]))\n",
    "        self.block_to_idx = {block: i for i, block in enumerate(self.emotion_blocks)}\n",
    "\n",
    "        # Create a list of all image paths and their emotion blocks\n",
    "        self.images = []\n",
    "        for cls in self.raw_classes:\n",
    "            class_path = os.path.join(root_dir, cls)\n",
    "            emotion_block = get_emotion_block(cls)\n",
    "            block_idx = self.block_to_idx[emotion_block]\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                self.images.append((img_path, block_idx, emotion_block, cls))  # Store raw class as well\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img1_block_idx, img1_block, img1_raw_class = self.images[idx]\n",
    "\n",
    "        # Decide whether to create a similar (same block) or dissimilar (different block) pair\n",
    "        should_get_same_block = random.random() < 0.5\n",
    "\n",
    "        if should_get_same_block:\n",
    "            # Find all images of the same emotion block\n",
    "            same_block_images = [(p, b, blk, r) for p, b, blk, r in self.images if b == img1_block_idx and p != img1_path]\n",
    "            if same_block_images:\n",
    "                img2_path, img2_block_idx, img2_block, img2_raw_class = random.choice(same_block_images)\n",
    "            else:\n",
    "                # If no other images in the same block, use the same image\n",
    "                img2_path, img2_block_idx, img2_block, img2_raw_class = img1_path, img1_block_idx, img1_block, img1_raw_class\n",
    "\n",
    "            # Label 1 indicates same emotion block\n",
    "            pair_label = torch.tensor(1.0, dtype=torch.float)\n",
    "        else:\n",
    "            # Find all images of different emotion blocks\n",
    "            diff_block_images = [(p, b, blk, r) for p, b, blk, r in self.images if b != img1_block_idx]\n",
    "            img2_path, img2_block_idx, img2_block, img2_raw_class = random.choice(diff_block_images)\n",
    "\n",
    "            # Label 0 indicates different emotion block\n",
    "            pair_label = torch.tensor(0.0, dtype=torch.float)\n",
    "\n",
    "        # Load images\n",
    "        img1 = Image.open(img1_path).convert('RGB')\n",
    "        img2 = Image.open(img2_path).convert('RGB')\n",
    "\n",
    "        # Apply transforms if available\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        # Return metadata for visualization\n",
    "        metadata = {\n",
    "            'img1_raw_class': img1_raw_class,\n",
    "            'img2_raw_class': img2_raw_class,\n",
    "            'img1_block': img1_block,\n",
    "            'img2_block': img2_block,\n",
    "            'img1_path': img1_path,\n",
    "            'img2_path': img2_path\n",
    "        }\n",
    "\n",
    "        return img1, img2, pair_label, metadata\n",
    "\n",
    "\n",
    "# Modified DataLoader to handle metadata\n",
    "class SiameseDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, num_workers=0, **kwargs):\n",
    "        super(SiameseDataLoader, self).__init__(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "            num_workers=num_workers, collate_fn=self.collate_fn, **kwargs\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imgs1 = []\n",
    "        imgs2 = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "\n",
    "        for img1, img2, label, meta in batch:\n",
    "            imgs1.append(img1)\n",
    "            imgs2.append(img2)\n",
    "            labels.append(label)\n",
    "            metadata.append(meta)\n",
    "\n",
    "        return torch.stack(imgs1), torch.stack(imgs2), torch.stack(labels), metadata\n",
    "\n",
    "\n",
    "# Siamese Network Base Class\n",
    "class SiameseBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        img1, img2, labels, _ = batch  # Ignore metadata for training\n",
    "        output = self(img1, img2)\n",
    "        \n",
    "        # Weighted binary cross entropy to give more importance to \"same\" class examples\n",
    "        # This helps the model to better recognize when images belong to the same block\n",
    "        weight_same = 2.0  # Give twice the importance to \"same\" class examples\n",
    "        weights = torch.ones_like(labels)\n",
    "        weights[labels == 1] = weight_same  # Apply higher weight to positive examples\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, labels, weight=weights)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        img1, img2, labels, _ = batch  # Ignore metadata for validation\n",
    "        output = self(img1, img2)\n",
    "        loss = F.binary_cross_entropy(output, labels)\n",
    "\n",
    "        # Use a lower threshold (0.4) to increase \"same\" class predictions\n",
    "        # This helps address the imbalance in the confusion matrix\n",
    "        threshold = 0.4\n",
    "        predictions = (output > threshold).float()\n",
    "        correct = torch.eq(predictions, labels).sum().item()\n",
    "        acc = correct / len(labels)\n",
    "\n",
    "        return {'val_loss': loss.detach(), 'val_acc': torch.tensor(acc, device=loss.device)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result, train_loss=None):\n",
    "        if train_loss:\n",
    "            print(\n",
    "                f\"Epoch {epoch}: train_loss: {train_loss:.4f}, val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f}\")\n",
    "\n",
    "\n",
    "# Siamese ResNet50 Model\n",
    "class SiameseResNet152(SiameseBase):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the feature extractor using ResNet50 without the final FC layer\n",
    "        resnet = models.resnet152(pretrained=pretrained)\n",
    "        modules = list(resnet.children())[:-1]  # Remove the final FC layer\n",
    "        self.feature_extractor = nn.Sequential(*modules)\n",
    "\n",
    "        # Freeze early layers for transfer learning\n",
    "        for param in list(self.feature_extractor.parameters())[:-30]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Layer to compute similarity\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        \"\"\"Forward pass for one input\"\"\"\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"Forward pass for the siamese network\"\"\"\n",
    "        # Extract features from both inputs\n",
    "        feat1 = self.forward_one(x1)\n",
    "        feat2 = self.forward_one(x2)\n",
    "\n",
    "        # Compute absolute difference between features\n",
    "        diff = torch.abs(feat1 - feat2)\n",
    "\n",
    "        # Compute similarity score\n",
    "        out = self.fc(diff)\n",
    "        return out.squeeze()\n",
    "\n",
    "\n",
    "# Evaluation function for validation\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "# Get learning rate from optimizer\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# Function to plot training and validation metrics\n",
    "def plot_metrics(history):\n",
    "    train_losses = [x.get('train_loss', 0) for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    val_accs = [x['val_acc'] for x in history]\n",
    "\n",
    "    # Create figure with 2 subplots\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses[1:], label='Train Loss')  # Skip the first one as it may not have train loss\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('siamese_training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Training function with one-cycle learning rate schedule\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()  # Clear GPU memory\n",
    "    history = []\n",
    "\n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "\n",
    "    # For tracking learning rates\n",
    "    lrs = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        # Create progress bar\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "\n",
    "        for batch in loop:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping if specified\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "\n",
    "        # Record training loss\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "\n",
    "        # Record learning rates for this epoch\n",
    "        result['lrs'] = lrs\n",
    "\n",
    "        # Print epoch results\n",
    "        model.epoch_end(epoch, result, result['train_loss'])\n",
    "\n",
    "        # Save history\n",
    "        history.append(result)\n",
    "\n",
    "        # Save model after each epoch\n",
    "        torch.save(model.state_dict(), 'latest_siamese_model.pth')\n",
    "\n",
    "        # Save best model if validation accuracy improves\n",
    "        if epoch == 0 or result['val_acc'] > max([h['val_acc'] for h in history[:-1]]):\n",
    "            torch.save(model.state_dict(), 'best_siamese_model.pth')\n",
    "            print(f\"Model saved at epoch {epoch} with val_acc: {result['val_acc']:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Function to evaluate the model on the test set with emotion blocks\n",
    "def evaluate_siamese_test_set(model, test_dl, emotion_blocks):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_metadata = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl, desc=\"Evaluating test set\"):\n",
    "            img1, img2, labels, metadata = batch\n",
    "            outputs = model(img1, img2)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_metadata.extend(metadata)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Use a lower threshold (0.4) to increase \"same\" class predictions\n",
    "    # This addresses the imbalance in the confusion matrix\n",
    "    threshold = 0.4\n",
    "    binary_preds = (all_preds > threshold).astype(int)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(all_labels, binary_preds)\n",
    "    print(f'Test Accuracy: {acc * 100:.2f}%')\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(all_labels, binary_preds, target_names=['Different', 'Same']))\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, binary_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=['Different', 'Same'],\n",
    "                yticklabels=['Different', 'Same'])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('siamese_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate ROC curve and AUC\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('siamese_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze emotion block performance\n",
    "    print(\"\\nEmotion block similarity analysis:\")\n",
    "\n",
    "    # Count correct predictions for each block pair\n",
    "    block_pair_counts = {}\n",
    "    block_pair_correct = {}\n",
    "\n",
    "    for i in range(len(all_labels)):\n",
    "        img1_block = all_metadata[i]['img1_block']\n",
    "        img2_block = all_metadata[i]['img2_block']\n",
    "        pair_key = f\"{img1_block}-{img2_block}\"\n",
    "\n",
    "        # Initialize if not seen before\n",
    "        if pair_key not in block_pair_counts:\n",
    "            block_pair_counts[pair_key] = 0\n",
    "            block_pair_correct[pair_key] = 0\n",
    "\n",
    "        block_pair_counts[pair_key] += 1\n",
    "        if all_labels[i] == binary_preds[i]:\n",
    "            block_pair_correct[pair_key] += 1\n",
    "\n",
    "    # Calculate and print accuracy for each block pair\n",
    "    print(\"\\nPair-wise accuracies:\")\n",
    "    block_pair_acc = {}\n",
    "    for pair, count in block_pair_counts.items():\n",
    "        if count > 0:\n",
    "            acc = block_pair_correct[pair] / count\n",
    "            block_pair_acc[pair] = acc\n",
    "            print(f\"{pair}: {acc:.4f} ({block_pair_correct[pair]}/{count})\")\n",
    "\n",
    "    # Create a matrix of similarities between blocks\n",
    "    similarity_matrix = np.zeros((len(emotion_blocks), len(emotion_blocks)))\n",
    "    block_to_idx = {block: i for i, block in enumerate(emotion_blocks)}\n",
    "\n",
    "    for i in range(len(all_metadata)):\n",
    "        if all_labels[i] == 1:  # Only for pairs that should be similar\n",
    "            block1 = all_metadata[i]['img1_block']\n",
    "            block2 = all_metadata[i]['img2_block']\n",
    "            similarity = all_preds[i]\n",
    "\n",
    "            idx1 = block_to_idx[block1]\n",
    "            idx2 = block_to_idx[block2]\n",
    "\n",
    "            # Update similarity (we'll average it later)\n",
    "            similarity_matrix[idx1, idx2] += similarity\n",
    "            similarity_matrix[idx2, idx1] += similarity  # Make it symmetric\n",
    "\n",
    "    # Average similarities\n",
    "    block_pair_counts_matrix = np.zeros((len(emotion_blocks), len(emotion_blocks)))\n",
    "    for i in range(len(all_metadata)):\n",
    "        if all_labels[i] == 1:\n",
    "            block1 = all_metadata[i]['img1_block']\n",
    "            block2 = all_metadata[i]['img2_block']\n",
    "            idx1 = block_to_idx[block1]\n",
    "            idx2 = block_to_idx[block2]\n",
    "            block_pair_counts_matrix[idx1, idx2] += 1\n",
    "            block_pair_counts_matrix[idx2, idx1] += 1\n",
    "\n",
    "    # Avoid division by zero\n",
    "    block_pair_counts_matrix = np.maximum(block_pair_counts_matrix, 1)\n",
    "    similarity_matrix = similarity_matrix / block_pair_counts_matrix\n",
    "\n",
    "    # Plot similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "                xticklabels=emotion_blocks, yticklabels=emotion_blocks)\n",
    "    plt.xlabel(\"Emotion Block\")\n",
    "    plt.ylabel(\"Emotion Block\")\n",
    "    plt.title(\"Emotion Block Similarity Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('emotion_block_similarity_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return acc, cm, roc_auc, similarity_matrix\n",
    "\n",
    "\n",
    "# Function to visualize sample pair predictions with emotion blocks\n",
    "def visualize_pair_predictions(model, test_dl, emotion_blocks, num_samples=5):\n",
    "    # Get a batch of image pairs\n",
    "    dataiter = iter(test_dl)\n",
    "    img1, img2, labels, metadata = next(dataiter)\n",
    "\n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img1, img2)\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "    # Convert tensors to CPU\n",
    "    img1 = img1.cpu()\n",
    "    img2 = img2.cpu()\n",
    "    labels = labels.cpu()\n",
    "    outputs = outputs.cpu()\n",
    "    preds = preds.cpu()\n",
    "\n",
    "    # Function to denormalize images for display\n",
    "    def denormalize(image, mean=stats[0], std=stats[1]):\n",
    "        img_denorm = image.clone()\n",
    "        for i in range(3):\n",
    "            img_denorm[i] = img_denorm[i] * std[i] + mean[i]\n",
    "        return torch.clamp(img_denorm, 0, 1)\n",
    "\n",
    "    # Plot the pairs with predictions\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(min(num_samples, len(img1))):\n",
    "        # Denormalize images\n",
    "        img1_display = denormalize(img1[i])\n",
    "        img2_display = denormalize(img2[i])\n",
    "\n",
    "        # Convert to numpy for matplotlib\n",
    "        img1_display = img1_display.permute(1, 2, 0).numpy()\n",
    "        img2_display = img2_display.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Plot image pair\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        plt.imshow(img1_display)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Image 1: {metadata[i]['img1_raw_class']}\\n({metadata[i]['img1_block']})\")\n",
    "\n",
    "        plt.subplot(2, num_samples, i + 1 + num_samples)\n",
    "        plt.imshow(img2_display)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Image 2: {metadata[i]['img2_raw_class']}\\n({metadata[i]['img2_block']})\")\n",
    "\n",
    "        # Add prediction information\n",
    "        true_label = \"Same\" if labels[i] == 1 else \"Different\"\n",
    "        pred_label = \"Same\" if preds[i] == 1 else \"Different\"\n",
    "        color = 'green' if labels[i] == preds[i] else 'red'\n",
    "        similarity_score = outputs[i].item()\n",
    "\n",
    "        plt.figtext(0.1 + (i * 0.2), 0.01,\n",
    "                    f\"True: {true_label}\\nPred: {pred_label}\\nScore: {similarity_score:.2f}\",\n",
    "                    color=color, ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.savefig('siamese_sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Required for Windows to avoid the multiprocessing error\n",
    "\n",
    "    # Create emotion block mapping\n",
    "    emotion_block_mapping = {emotion: get_emotion_block(emotion) for emotion in raw_classes}\n",
    "    \n",
    "    # Get unique emotion blocks\n",
    "    emotion_blocks = list(set(emotion_block_mapping.values()))\n",
    "    \n",
    "    # Create Siamese datasets with metadata and emotion blocks\n",
    "    train_ds = SiameseDataset(train_dir, train_transforms, emotion_block_mapping)\n",
    "    test_ds = SiameseDataset(test_dir, test_transforms, emotion_block_mapping)\n",
    "\n",
    "    # Create data loaders with custom collate function\n",
    "    batch_size = 64\n",
    "    train_dl = SiameseDataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_dl = SiameseDataLoader(test_ds, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Move data to device\n",
    "    train_dl = DeviceDataLoader(train_dl, device)\n",
    "    test_dl = DeviceDataLoader(test_dl, device)\n",
    "\n",
    "    # Create Siamese model\n",
    "    model = SiameseResNet152(pretrained=True)\n",
    "    model = to_device(model, device)\n",
    "\n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")\n",
    "\n",
    "    # Get initial validation metrics\n",
    "    initial_result = evaluate(model, test_dl)\n",
    "    print(\"Initial validation metrics:\", initial_result)\n",
    "\n",
    "    # Train the model\n",
    "    history = fit_one_cycle(\n",
    "        epochs=30,  # Reduced epochs for siamese network\n",
    "        max_lr=0.001,\n",
    "        model=model,\n",
    "        train_loader=train_dl,\n",
    "        val_loader=test_dl,\n",
    "        weight_decay=0.01,\n",
    "        grad_clip=0.1,\n",
    "        opt_func=torch.optim.Adam\n",
    "    )\n",
    "\n",
    "    # Plot training metrics\n",
    "    plot_metrics(history)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'final_siamese_model.pth')\n",
    "\n",
    "    # Load the best model for evaluation\n",
    "    model.load_state_dict(torch.load('best_siamese_model.pth'))\n",
    "    model = to_device(model, device)\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    test_acc, conf_matrix, roc_auc, similarity_matrix = evaluate_siamese_test_set(model, test_dl, emotion_blocks)\n",
    "\n",
    "    # Visualize some predictions with emotion blocks\n",
    "    visualize_pair_predictions(model, test_dl, emotion_blocks, num_samples=5)\n",
    "\n",
    "    # Plot learning rate vs. loss\n",
    "    if len(history) > 0 and 'lrs' in history[0]:\n",
    "        # Extract learning rates and losses\n",
    "        epochs_lrs = []\n",
    "        epochs_losses = []\n",
    "\n",
    "        for epoch_result in history:\n",
    "            if 'lrs' in epoch_result and 'train_loss' in epoch_result:\n",
    "                # We'll use the average LR for the epoch\n",
    "                avg_lr = sum(epoch_result['lrs']) / len(epoch_result['lrs'])\n",
    "                epochs_lrs.append(avg_lr)\n",
    "                epochs_losses.append(epoch_result['train_loss'])\n",
    "\n",
    "        # Plot LR vs. Loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs_lrs, epochs_losses, 'o-')\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate (log scale)')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title('Learning Rate vs. Training Loss')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.savefig('siamese_lr_vs_loss.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This is the key to fixing the multiprocessing error\n",
    "    multiprocessing.freeze_support()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# child net dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\"Emotion class similarity analysis:\n",
    "\n",
    "Pair-wise accuracies:\n",
    "Angry-Happiness: 1.0000 (2/2)\n",
    "Angry-Angry: 0.0000 (0/4)\n",
    "Angry-Disgust: 1.0000 (1/1)\n",
    "Angry-Sad: 1.0000 (1/1)\n",
    "Angry-Surprised: 1.0000 (1/1)\n",
    "Angry-Fear: 1.0000 (1/1)\n",
    "Angry-Neutral: 1.0000 (1/1)\n",
    "Disgust-Disgust: 0.0000 (0/6)\n",
    "Disgust-Fear: 1.0000 (1/1)\n",
    "Disgust-Happiness: 1.0000 (1/1)\n",
    "Disgust-Sad: 1.0000 (1/1)\n",
    "Fear-Fear: 0.0000 (0/5)\n",
    "Fear-Disgust: 1.0000 (1/1)\n",
    "Fear-Happiness: 1.0000 (1/1)\n",
    "Happiness-Disgust: 1.0000 (1/1)\n",
    "Happiness-Happiness: 0.0000 (0/12)\n",
    "Happiness-Angry: 1.0000 (5/5)\n",
    "Happiness-Fear: 1.0000 (1/1)\n",
    "Happiness-Surprised: 1.0000 (1/1)\n",
    "Neutral-Happiness: 1.0000 (2/2)\n",
    "Neutral-Neutral: 0.0000 (0/5)\n",
    "Neutral-Disgust: 1.0000 (2/2)\n",
    "Neutral-Fear: 1.0000 (1/1)\n",
    "Neutral-Surprised: 1.0000 (1/1)\n",
    "Sad-Sad: 0.0000 (0/9)\n",
    "Sad-Disgust: 1.0000 (2/2)\n",
    "Sad-Happiness: 1.0000 (1/1)\n",
    "Sad-Surprised: 1.0000 (2/2)\n",
    "Surprised-Angry: 1.0000 (3/3)\n",
    "Surprised-Fear: 1.0000 (1/1)\n",
    "Surprised-Surprised: 0.0000 (0/7)\n",
    "Surprised-Sad: 1.0000 (4/4)\n",
    "Surprised-Happiness: 1.0000 (1/1)\n",
    "Surprised-Neutral: 1.0000 (2/2)\n",
    "Surprised-Disgust: 1.0000 (1/1)\"\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
